---
layout: post
title: "Windows 소켓 프로그래밍"
date: 2024-11-29
---

# 기초

## 작업자 스레드 생성 

- Thread : 실행 단위 

	- CPU라는 전산자원을 소모하는 단위

	- 멀티 스레드 = main 함수가 n 개 있다 , 여러 개가 동시 작동 가능 

	- Thread는 개별화된 흐름(문맥)과 전용스택(1MB 스택 메모리, 어쨌든 프로세스에게 주어진 VMS의 일부)을 갖는 실행의 단위

	- 모든 Thread는 자신이 속한 프로세스의 가상 메모리 공간을 공유 

- Process : Thread를 묶은 집합체

	- 운영체제는 새로운 프로세스를 생성한다 = 실행되는 것 ← 전산자원 줌, 권한도 줌 

	- 프로세스가 파일 개방할 수 있는 권한 있냐 없냐로 통제

	- 가상메모리 공간 할당 (cpu + Memory)

	- 프로세스는 스레드 생성하게 되어 있음 (= main thread)

		- 작업자 스레드 생성 ← CreateThread()

		- 기존의 메인 스레드인 T1, 새로 생성된 작업자 스레드인 T2의 흐름이 **공존**함 

		- cpu 코어가 여러개 → 연산 흐름 공존 

		- **동기화 **

			방법1. 진입제어 (Queue같은 경우, 동시 접근하면 race condition 자동발생 → 문제발생 → 통제필요 → Critical Section로 스레드 진입 통제 ⇒ 메모리 동시 접근 제어, … )

			방법2. 시점 (메인 스레드에서 T2가 종료되는 시점을 인지하고 싶다면? Event 사용 → **신호 → 신호의 변화를 감지해서 흐름 맞춤)**

- 스레드 동기화 

	- Critical section : 진입통제

	- Event : 시점

<br/>

### 작업자 스레드 생성 및 동기화


```c++
DWORD dwThreadId = 0;
// 새로운 스레드를 생성한다. 
HANDLE hThread = ::CreateThread(
								NULL,           // 보안속성 상속
								0,              // 스택 메모리는 기본 크기(1MB)
								ThreadFunction, // 스레드로 실행할 함수이름
								NULL,           // 함수에 전달할 매개변수 
								0,              // 생성 플래그는 기본값 사용
								&dwThreadId);   // 생성된 스레드 ID 저장
								
								
HANDLE CreateThread(
	[in, optional]  LPSECURITY_ATTRIBUTES   lpThreadAttributes,
	[in]            SIZE_T                  dwStackSize,
	[in]            LPTHREAD_START_ROUTINE  lpStartAddress,
	[in, optional]  __drv_aliasesMem LPVOID lpParameter,
	[in]            DWORD                   dwCreationFlags,
	[out, optional] LPDWORD                 lpThreadId
);
```

- HANDLE은 대부분 int 거나, void*(포인터)

- Process, Thread는 고유 ID 갖고 있음 (LPDWORD = 쉽게 말해 unsigned int…

- SIZE_T 에 0 전달하면, 컴파일러가 정한 기본값 사용

- lpStartAddress : 새로운 스레드로 만들어져서 실행될 스레드에서 메인 역할을 할 함수의 주소 전달해 

- NULL이면, 보안속성 상속 : CreateThread 함수를 호출한 스레드의 보안 속성을 상속받겠다

- 넘어온 핸들, hTrhead는 반드시 closeHandle(HANDLE hThread)로 닫아주기 

<br/>

**스레드 상태**

- Run : 작동 중

- Suspend 

- Resume

- Sleep 

- Alertable 

- **Alertable wait : Resume 되어서 깨어날 수 있는 대기 상태 **

→ thread는, suspend (자고 있음, 일시정지) 상태면, resume(suspend에 의해 일시정지된 스레드를 실행대기상태로 만듦)으로 깨워야 함 

<br/>

## 작업자 스레드 동기화 

- Event : Kernel Object


```c++
// 이벤트 객체를 생성한다.
HANDLE hEvent = ::CreateEvent(
		NULL, // 디폴트 보안 속성 적용
		FALSE, // 자동으로 상태 전환
		FALSE, // 초기상태는 FALSE
		NULL); // 이름 없음 
```

- 여러 프로세스에서 접근할 가능성이 있을 때만 이름 부여함, 이름 없음 = 프로세스가 하나만 등장 할 때… 

- Event 상태

	- set / reset

	- set : 1 , reset: 0

	- Event 오브젝트에 대해 set하거나, reset함

	- 깃발 올려내려

	- Event : 이벤트를 감시하는 주체 등장함 (Wait) 

	- Wait for ~를 감시 

	- Set되면 반응함 

- 자동으로 상태전환 1 : 감시하는 애가 인지했다 = reset으로 바꿔버림 / 0이면, 바뀐 상태 그대로 유지 

<br/>


```c++
BOOL SetEvent(
	[in] HANDLE hEvent
);

::SetEvent((HANDLE)pParam);
```

<br/>

**작업자 스레드 생성 및 동기화**


```c++
for(int i=0; i<5; ++i)
{
	printf("~");
	if (i==3 && 
			::WaitForSingleObject(hEvent, INFINTIE) == WAIT_OBJECT_0) // i가 3이면 이벤트가 set 되길 무한정 대기 
	{
		puts("종료 이벤트 감지!");
		::CloseHandle(hEvent);
		hEvent = NULL;
	}
} 
```

<br/>


```c++
DWORD WaitForSingleobject(
	[in] HANDLE hHandle, // Event 객체에 대한 핸들 (대기할 대상) 
	[in] DWORD  dwMilliseconds
);
```

⇒ wait과 Set 하는 스레드는 분리되어 있겠지 

<br/>

- main에서 Thread T1 생성, 각자 흘러가다가 이벤트 생성하면, main에선 Wait 

- Wait이 set 되는 시점을 인지해서 다음 코드로 정해진 일 수행 

- waitforsingle.. 이 먼저 실행되었다면 대기 걸릴거고, 이미 set이 실행 끝났다면 굳이 기다릴 필욘 없겠지 

- 각자의 흐름을 어느 지점에서 신호를 통해 일치시킬 때의 예제… 

<br/>

<br/>

# 소켓 프로그래밍

## 소켓의 본질에 대한 이해 

- 소켓의 본질 =** **<span style='color:red'>**File **</span>

- File ? 

	- Data..동시에 운영체제 커널에 구현되어 있는 추상화된 인터페이스 

- Socket? 

	- OS 커널에 구현되어 있는 프로토콜 요소에 대한 추상화된 인터페이스 

	- fopen_s(&fp, “CON”, “w”); → 파일 포인터가 가리키는 대상이 콘솔, 디바이스 파일 

	- 장치 파일의 일종 

	- 파일에 대해 write하면 콘솔을 가리키니까 화면으로 모든 출력 나감

	- putchar() : 글자 하나가 콘솔로 나감 

<br/>

- 파일에 대한 입출력 → 주체는 프로세스가 행위의 주체 (입출력행위) 

- 프로세스가 대상체가되는 파일에 대해 open, create, close, delete, RWX(읽쓰실행)

- 대상체 파일이 TCP 파일에 대한 추상화된 인터페이스면 TCP Socket이 됨

- 소켓프로그래밍 = TCP라는 대상을 추상화시킨 파일에 대한 입출력 방법론 

- 소켓에 대해 주체인 프로세스가 오픈, 생성 가능, 

- Write는 send 

- Read는 Receive

- 파일은 fopen, fclose, read, write

- 소켓함수로 여닫아

	- TCP 프로토콜 특징때문에 정해진 흐름대로 함수 호출해야 하는데 그 외는 파일과 똑같음

	- 소켓 = 파일! 

- 데이터 단위? 

	- **Stream **: 연속된 Packet, segment의 집합  

	- 유저모드 어플리케이션에서 소켓 나오면 스트림 같이 나와야 함

	- 커널로 내려가면서 데이터 단위가 바뀜. **L4(TCP)**에서 쓰는 단위 **세그먼트**, **L3(IP)**는 **패킷 **

	- Segment : Stream을 일정 단위로 자른 조각 하나 

	- Packet : 잘 포장한 Segment 

	- Segmentation : Stream을 자르는 행위 

<br/>

## TCP (연결) 상태 다이어그램과 전이 

- TCP = Connection Oriented Protocol VS UDP = Connection X

- TCP는 안정성에 대해 논함

	- host 하나 있는데, 다른 호스트에서 데이터 보냄 → 인터넷으로 데이터 전달 → 유통되는 데이터 단위가 패킷 

	- 데이터가 하나, 둘, 셋… 가는데 문제는 3번이 loss될 수도 있음

	- TCP는 운영체제 수준에서 kernel에 구현되어 있는데, 커널이 3번 다시 보내달라고 하든지 .. 

	- UDP는 재전송이 없음. 원한다면 알아서 구현해 

- 연결 절차 = 3-Way HandShake 로 연결함

	- 계약서 주고받고와 유사

	- 연결 = 상태가 무조건 딸려나옴

	- 연결을 하기 전, 후 → 전 상태 -중-완료 상태 .., 송수신 가능 상태, 연결 종료 과정 .. 종료 요청 상태 등 상태는 계속해서 **전이**

	- 상태 전이의 적절성? 

		- 벨이 울려야 전화를 받아 → 벨이 울려야 받기 가능 → 전이 적절 

![스크린샷_2024-11-29_141033.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/83a69d8f-6be2-4385-96df-7c58b0b2f3bf/cefabbd9-bafe-498b-a745-9d138e7f2489/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7_2024-11-29_141033.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB4667UCDAO56%2F20251225%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251225T154707Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHYaCXVzLXdlc3QtMiJIMEYCIQC%2BDG0DI7nH5vSbSatF4ThrdxnL5PcpVZW1R8Nx%2Bepq6QIhAJQzi0aooV9XvdnCsIlmemMMC%2BeFVeUIj0oqiAivoFUMKv8DCD8QABoMNjM3NDIzMTgzODA1Igw2lRkHAAgYlviLsTUq3APRpFpjVVz1hhb5UkSi%2FTShsHfafpVw%2Fdx91W0hY7jFiFq4HvEqLj96vHhQ5%2BsG27gm1kk9TlNwzqylUIO6YhTy01NOE8e8VrLZDuSimDbSAO4JdMTcbpT2rlPBOjbx5v6pHRM6Np05Wnr2HEOm7wBw%2BwhjjfWVrx%2BziSKx8YJMGpDe2sT3MGfI5u8C1v2PbCjJ%2BgUzcA%2FhoeEUIjLHyvxoFu3J9IrPy34%2BttVRe9wGJFcNBuaaqJm6bHFmatM10QKM3Cy6xmoEjBV2ObIHJmkuRJE%2BExa6OgB%2Fnmd4Z7txSRLn3qk14m6Dl%2BTdJNwWQ1KPNlvzq7TXtYFKg8s8dB3n3TUBWA5IfR0haCsOivDj5D1W5RVEMxooBTB6kXy3dHvAxQm87fMvotHT0eKK4VnpuEI6BxOFtUQn7CRsF9cwFIy7K27w1G1xJXW5LrDPippgxamnyhbjWEdnrn1pKrxA6zBErHkBJS1%2FRLWCQHsEbLLggF%2FknWHCdUtCPpY42tHYlJ%2FUlgMsyW%2B2VWsfFZnYNLQJ0v9giTOW0gCIe3JiXOf83xomiWxjPGZnqx3xZyP8F1HbON1SIWxwDBbi54XxetqJS7dCarx%2FDaWiNy13tKKsgN4vjc7QIdbTqzCxjrXKBjqkAUFoIv4xmNAfhz6rTkvLj6xemGKbDHfrwofRnU%2FI%2Bg2%2BRMejQrMXgs57ypainBrD7XV%2BbRatpzzVOQAXLWxY8j8WyDwZIs%2F526tPPneNwcmuD1vZRJJQtUAyzo%2B6J4Obc%2BiXQIDb3oc2RugRAcNI6nM82p8htZijBi5WsTpiHC%2F9aoCq3k0KNbQbsWka%2FKnPjHRfvQDJ8w5iNtk3U0plvy5aC6K%2B&X-Amz-Signature=120887ef15d433639530564072391ac0211452bea834e2a140de4846f1dcfb06&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject)

- 소켓의 본질 파일

	- 개수가 정해져 있음 (소켓 사용 가능 개수) 

	- 소켓 → 처음에 없어 = 닫혀있는걸 열어

	- 이미 만들어져있는게 아님

	- 회색 박스 = 상태 

	- Client, Server 간에 … 

	- 먼저 서버는 대기해야 함 → 누군가 접속해야하니까 

	- 클라가 연결 시도 

	- 서버가 연결 받아들이면 연결 받음! 

	- 서버 연결 대기 = 수동적, 클라는 액티브하게 먼저 연결

	- 대기 중인 상태, 연결 시도하는 상태, 연결 된 상태 … 상태 전환

	- LISTEN : 대기상태 

	- SYN RECEIVED : 연결하자! 

	- 파란 라인 = 서버 

	- 클라이언트는 소켓 열어서 서버에 연결하자 → 연결됨 → 연결 끊는건 클라이언트 

	- TIME WAIT : 소켓 닫힐때 까지 시간 좀 걸림. 서버는 안되고 클라이언트가 타는 흐름 

	- tcp 연결을 전화로 생각하면 쉽다. 

	- 전화 걸기 전, 걸기, 대기, 연결, 통화, 연결 끊기

	- 끊는건 클라이언트가 되도록

	- 위가 프로토콜 설계 

	 

Socket 함수들 = API, API Full kit → SDK

이론 확인은 WireShark로 

<br/>

상태라는 정보들이 모두 TCP Segment에 들어 있음 → 소켓 프로그래밍할 땐 볼 방법이 없음 → 직접 보려면 wireshark 사용해야만 함

상태 다이어그램은 네트웤 공부 따로 하자 

<br/>

## TCP 에코 서비스 전체 흐름 

- 함수 + **순서**

- Client, Server 서로 코드 상호작용 → 절차적 흐름 타임라인에서 어디가 동기화되고 어떤 절차 지켜야 하고.. 

- TCP 소켓 프로그래밍 예제 1. 에코 

	- 메아리 ? → 에코 서버에 클라이언트가 hello 보냄 → 서버가 그대로 hello라고 보냄 

	- 가장 최소한… 

- file 쓰기 위해선 fopen → fread(), fwrite() → fclose()

- 소켓도 동일.

![스크린샷_2024-11-29_142506.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/83a69d8f-6be2-4385-96df-7c58b0b2f3bf/e4deb554-7720-4922-a1fc-c26d97544120/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7_2024-11-29_142506.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB4667UCDAO56%2F20251225%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251225T154707Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHYaCXVzLXdlc3QtMiJIMEYCIQC%2BDG0DI7nH5vSbSatF4ThrdxnL5PcpVZW1R8Nx%2Bepq6QIhAJQzi0aooV9XvdnCsIlmemMMC%2BeFVeUIj0oqiAivoFUMKv8DCD8QABoMNjM3NDIzMTgzODA1Igw2lRkHAAgYlviLsTUq3APRpFpjVVz1hhb5UkSi%2FTShsHfafpVw%2Fdx91W0hY7jFiFq4HvEqLj96vHhQ5%2BsG27gm1kk9TlNwzqylUIO6YhTy01NOE8e8VrLZDuSimDbSAO4JdMTcbpT2rlPBOjbx5v6pHRM6Np05Wnr2HEOm7wBw%2BwhjjfWVrx%2BziSKx8YJMGpDe2sT3MGfI5u8C1v2PbCjJ%2BgUzcA%2FhoeEUIjLHyvxoFu3J9IrPy34%2BttVRe9wGJFcNBuaaqJm6bHFmatM10QKM3Cy6xmoEjBV2ObIHJmkuRJE%2BExa6OgB%2Fnmd4Z7txSRLn3qk14m6Dl%2BTdJNwWQ1KPNlvzq7TXtYFKg8s8dB3n3TUBWA5IfR0haCsOivDj5D1W5RVEMxooBTB6kXy3dHvAxQm87fMvotHT0eKK4VnpuEI6BxOFtUQn7CRsF9cwFIy7K27w1G1xJXW5LrDPippgxamnyhbjWEdnrn1pKrxA6zBErHkBJS1%2FRLWCQHsEbLLggF%2FknWHCdUtCPpY42tHYlJ%2FUlgMsyW%2B2VWsfFZnYNLQJ0v9giTOW0gCIe3JiXOf83xomiWxjPGZnqx3xZyP8F1HbON1SIWxwDBbi54XxetqJS7dCarx%2FDaWiNy13tKKsgN4vjc7QIdbTqzCxjrXKBjqkAUFoIv4xmNAfhz6rTkvLj6xemGKbDHfrwofRnU%2FI%2Bg2%2BRMejQrMXgs57ypainBrD7XV%2BbRatpzzVOQAXLWxY8j8WyDwZIs%2F526tPPneNwcmuD1vZRJJQtUAyzo%2B6J4Obc%2BiXQIDb3oc2RugRAcNI6nM82p8htZijBi5WsTpiHC%2F9aoCq3k0KNbQbsWka%2FKnPjHRfvQDJ8w5iNtk3U0plvy5aC6K%2B&X-Amz-Signature=23fcf340001e41063160eafc09a38426745f5384c441fb631f1a47907550d33f&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject)

- fopen = socket()→bind()→listen()

- bind() : TCP라는 프로토콜에 해당하는 정보를 파일에 붙여주기 

	- IP주소 + 포트번호 를 소켓에 붙여주기 

- listen() : 상태 변경됨. listen 상태 → accept 까지 가야 됨 

	- 대기 중인 상태일 때, 클라이언트에서는 socket으로 열고, connect함. → 이때 연결하자 하면 서버가 accept 

	- accept도 socket 하나 만들어서 받음, 그 소켓이 recv로 감

- 소켓 종류 

	1. 서버의 접속대기용 소켓

	1. 서버의 통신 소켓 (accpet의 결과 소켓, 클라이언트와 통신하는 소켓)

	1. 클라이언트 소켓 

	- 소켓이 종류가 대략 서버 소켓(접속대기용) → 서버 통신 소켓 (accpet의 결과소켓, 클라이언트랑 통신하는 소켓) 

	- 클라이언트가 만드는건 클라이언트 소켓   

- 서버 연결되면 send, recv 함

- 클라이언트가 셧다운하면 → 너랑 이제 통신 안할거야 선언→ 연결 끊기 위한 흐름으로 진입

- closesocket() : 소켓 닫혀서 통신 안함

- 서버 입장에선 대기하고 있다가 끊자고 하니까 끊는 흐름 진입 

- 서버가 클라와 통신하는 소켓만 닫아 → 대기 소켓은 사라지지 않음 - 따로 또 닫아줘야함

<br/>

## TCP 에코서버 제작: 서버 접속대기 

### 1. server - 접속 대기 소켓 생성 


```c++
SOCKET hSocket = ::socket(AF_INET, SOCK_STREAM, 0);
if (hSocket == INVALID_SOCKET)
{
	puts("ERROR: 접속 대기 소켓을 생성할 수 없습니다.");
	return 0;
}
```

- socket() : 소켓 핸들 반환

	- AF_INET : L3(IP 프로토콜), INET=인터넷

	- SOCK_STREAM : L4 (TCP)

<br/>

### 2. server - 포트 바인드


```c++
SOCKADDR_IN svraddr = { 0 };
svraddr.sin_family = AF_INET; 
svraddr.sin_port = htons(25000);
svraddr.sin_addr.S_un.S_addr = htonl(INADDR_ANY);
if (::bind(hSocket, (SOCKADDR*)&svraddr, sizeof(svraddr))
	== SOCKET_ERROR)
{
	puts("ERROR: 소켓에 IP 주소와 포트를 바인드할 수 없습니다.");
	return 0;
}
```

- 포트 번호, 주소 

	- 포트번호 : TCP Port

	- Address : IP 주소 

	- IP 주소는 호스트에 붙음. 호스트=인터넷에 연결된 컴퓨터

	- IP 주소는 IP에 붙는 식별자 

	- NIC에 붙는게 하드웨어주소 (MAC 주소)

	- port 번호는 tcp 

	- 소켓 = 유저모드 어플리케이션 프로세스가 커널에 TCP 스택을 추상화한 인터페이스를 타고 입출력함. 소켓이 입출력하려면 포트번호, ip 주소 알아야 함

	- 컴퓨터는 공유기로부터 IP 주소를 받아서 갖고 있음

	- IP 주소는 cmd에서 ipconfig라고 입력하면 알 수 있음 (설정되어 들어가있는 값) = 127.0.0.1 쓰면 동일. 나를 뜻함

	- INADDR_ANY : IP 주소가 뭐가 되었든 상관하지 않는다.

		- 컴에 IP 주소 여러개일 수 있음. NIC이 여러개면 가능

		- 서버에 누군가가 접속해야하는데, 클라이언트는 반드시 서버의 IP 주소와 포트 번호 알아야 접속 가능. 포트 번호는 확정인데, IP 주소는 여러 개 중 어떤거든 다 접속하게 하고 싶다고 하면, INADDR_ANY라고 씀 

		- 만약 특정 IP만 접속 가능하게 하고 싶으면 다른거 사용해야 함

		- ip 주소를 여러 개 할당하는 경우 간혹 있음

- 포트 번호 ? 

	- 함수 실패하는 경우에… 

	- bind해서 소켓 파일에 묶어야 하는데 왜 실패할까 

	- 내가 쓸 수 있는 ip 있어야 하는데, 엉뚱한거 바인딩하거나, 포트를 바인드할 수 없는 경우 

	- 프로세스 A, B → A가 TCP 25000을 이미 열어버렸다. 이 소켓을 사용하고 잇는 A, B가 근데 25000 하고 싶다고 bind 하면 에러 발생

	- 이미 누군가가 사용하고 있어 → bind 실패 

	- ipconfig + netstate + ano 를 사용하면 어떤 포트가 열려있는지 알 수 있음 

	- 바인드 = 소켓에 IP, 포트번호 묶기 

	- **포트 번호는, 소켓 하나를 식별한다. 소켓은 프로세스가 연다. 즉, 포트 번호는 프로세스에 대한 식별자가 된다.** 

<br/>

### 3. server - 포트 바인드


```c++
// socket address 구조체
typedef struct sockaddr_in {

#if(_WIN32_WINNT < 0x0600)
	short sin_family; 
#else //(_WIN32_WINNT < 0x0600)
	ADDRESS_FAMILY sin_family;
#endif //(_WIN32_WINNT < 0x0600)

	USHORT sin_port; // 16bit=2^16
	IN_ADDR sin_addr; // 32bit
	CHAR sin_zero[8];
} SOCKADDR_IN, *PSOCKADDR_IN;
```

- Bind 하려면 IP, Port 묶어야 하는데 모두 숫자다. → 필요한 형식에 맞게 변환하는데 htons htonl..

<br/>

### 4. server -접속 대기 

서버 소켓이 Closed 상태였다가 Listen으로 전환 


```c++
if (::listen(hSocket, SOMAXCONN) == SOCKET_ERROR)
{
	puts("ERROR: 리슨 상태로 전환할 수 없습니다.");
	return 0;
}
```


```c++
int WSAAPI listen(
	[in] SOCKET s, // listen 상태로 전환할 소켓 핸들 
	[in] int backlog 
);
```

→ 연결 요청 받을 수 있는 상태까지 왔다. 

<br/>

### 5. 클라이언트 연결 받기 및 통신 


```c++
// 클라이언트 접속 처리 및 대응 
SOCKADDR_IN clientaddr = { 0 };
int nAddrLen = sizeof(clientaddr);
SOCKET hClient = 0;
char szBuffer[128] = { 0 };
int nReceive = 0; // 수신한 바이트 
```

서버 소켓의 생성 완료 후 listen 상태가 됨 

→ 이 서버에 대고 클라이언트가 접속할 수 있는 것, 접속 = connect()

호출하면 서버 소켓이 리슨 상태였으면 accept로 이어질 수 있는 것

<br/>

소켓

1. 서버가 접속 받기 위한 서버소켓

1. 클라이언트랑 통신하기 위한 서버의 통신소켓 

1. 클라이언트 소켓 

<br/>

**클라이언트 접속**


```c++
// 클라이언트 연결을 받아들이고 새로운 소켓 생성 (개방)
while ((hClient = ::accept(hSocket, (SOCKADDR*)&clientaddr, &nAddrLen)) != INVALID_SOCKET) 
{
	puts("새 클라이언트가 연결되었습니다.");
```

서버의 소켓 생성 → socket(), accept() 

연결을 받아서 소켓 반환하는 함수 accept

hClient : 서버 입장에서  Remote Client와 통신하기 위한 소켓 

SOCKADDR ← clientaddr에 나에게 접속한 Remote(호스트) 정보가 있음

서버는 연결 받고 클라가 다시 끊고 붙고 하고 여럿 붙으면 다 대응하니까 accept는 while에 넣음

에코에선 while에 넣은게 큰 의미는 없어

<br/>

**수신 및 Echo**


```c++
// 클라이언트로부터 문자열 수신
while ((nReceive = ::recv(hClient, szBuffer, sizeof(szBuffer), 0))
{
	// 수신한 문자열을 그대로 반향전송
	::send(hClient, szBuffer, sizeof(szBuffer), 0);
	puts(szBuffer);
	memset(szBuffer, 0, sizeof(szBuffer));
	}
```

받아서 그대로 보냄

recv 는 데이터 읽고 send는 write

recv 호출될 때 통신소켓(hClient), 메모리 주소(버퍼)와 메모리 크기 같이 넘김

→ 서버입장에서, 소켓이 리슨하는 소켓이라면 → 접속만 받는 소켓이면 accept 하면 새 소켓 생겨

→ 새로 생긴 통신 소켓으로 클라랑 통신함 

⇒ szBuffer로 메모리 할당했었어. 128바이트 → 이 버퍼로 recv 해줘 → 나에게 128 바이트만큼의 여유공간있어. 읽을 때 128 바이트를 읽겠다고 했어. sizeof 

→ 근데 클라에서 다섯글자 보냈으면 용량 얼마 안됨. 저장하는데 문제 없어

→ 128 바이트만큼 수신하려고 기다렸는데 실제론 5바이트 들어오니까 nReceive 는 5, 근데 다시 보낼 땐 128 바이트를 다 보냄 (버퍼 그대로 전체보내기) 

→ TCP, IP에도 입출력 버퍼 있어 → Buffered I/O → 네트워크로 정보 유입되어서 올라올 때 TCP 만나서 분해된 애들을 조립하는데 5바이트만큼 조립해서 버퍼에 넣어. 넣은걸 Buffer에 복사함

<br/>

### **6. 연결 종료**


```c++
// 클라이언트가 연결 종료함
	::shutdown(hSocket, SD_BOTH);
	::closesocket(hClient);
	puts("클라이언트 연결이 끊겼습니다.");
}
```

- shutdown (서버리슨소켓, SD_BOTH) 

	- 송수신 차단, 서버소켓의 송수신 차단

- 클라이언트 서버도 닫아서 연결 종료 

- 위의 while에 있는 recv 함수는 데이터가 날아올 때 까지 기다린다. 이때 0을 반환하면 클라이언트가 연결을 끊었다는 의미

<br/>


```c++
// 리슨 소켓 닫기 
::closesocket(hSocket);
```

서버의 리슨 소켓도 닫아서 서버 종료 

<br/>

<br/>

**초기세팅**


```c++
WSADATA wsa = { 0 };
if (::WSAStartup(MAKERWORD(2, 2), &wsa) != 0) 
{
	puts("ERROR: 윈속을 초기화할 수 없습니다.");
	return 0;
}
```

윈도우 소켓 API 

startup 해서 윈속 라이브러리 초기화해서 로딩하는 것 

윈도우에만 있음

<br/>

[ 순서 ]

1. 접속 대기 소켓 생성

1. 포트 바인딩

1. 접속 대기 상태로 전환

1. 클라이언트 접속 처리 및 대응

	1. 클라이언트 연결을 받아들이고 새로운 소켓 생성(개방)

	1. 클라이언트로부터 문자열 수신

	1. 수신한 문자열 그대로 반향전송 (Echoing) 

	1. 클라이언트가 연결 종료 

1. 리슨 소켓 닫기 

1. 윈속 해제 

<br/>

처음 소켓 프로그래밍 할 때 순서가 헷갈리고, 클라도 같이 생각해야하니까 흐름을 정확히 이해하자. 

<br/>

<br/>

## TCP 에코 클라이언트 제작 

### 1. 클라이언트 소켓 생성


```c++
SOCKET hSocket = ::socket(AF_INET, SOCK_STREAM, 0);
if (hSocket == INVALID_SOCKET)
{
	puts("ERROR : 소켓을 생성할 수 없습니다.");
	return 0;
}
```

<br/>

### 2. 포트 바인드 및 연결


```c++
SOCKADDR_IN svraddr = { 0 };
svraddr.sin_family = AF_INET;
svraddr.sin_port = htons(25000);
svraddr.sin_addr.S_un.S_addr = inet_addr("192.168.66.1");
if (::connect(hSocket, (SOCKADDR*)&svraddr, sizeof(svraddr)) == SOCKET_ERROR)
{
	puts("ERROR : 서버에 연결할 수 없습니다.");
	return 0;
}
```

- 소켓 만들고 connect 바로 하는데, 서버의 정보 알아야 연결할 수 있어 (IP, Port) 

- 연결하기에 앞서, sin_port, sin_addr .. 

- inet_addr : 문자열로 쓰면 구조에 맞는 숫자로 변환해줌 (서버 주소) 

	- 컴 하나면 127.0.0.1로 쓰면 됨 

- connect 하는데, svraddr (접속할 서버의 정보) 클라이언트는 포트 안여나요? 열어요 → 클라 포트를 왜 안써요? → 클라 포트는 운영체제가 임의로 지정함

	- 내가 무슨 포트 열어서 접속하겠어 안해도 됨

- 클라 소켓에선 포트 번호 지정 안하고, 운영체제가 할당해줌

<br/>

### TCP 연결 과정 (3-way handshaking) 

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/83a69d8f-6be2-4385-96df-7c58b0b2f3bf/d112487e-8630-4c6e-b5e3-4cd779975560/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466QZSENN3K%2F20251225%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251225T154707Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHYaCXVzLXdlc3QtMiJIMEYCIQCqzRaVHG4KNKdUHjsDnQ1Zhysytv2Qu6mloOu5PHdebwIhAIaqPwXoQ9uffG7%2F1bKDM2cgzRts6PS7iHxeBRVBcjLJKv8DCD8QABoMNjM3NDIzMTgzODA1Igz%2BwoJ7g9bX83yWpPQq3APgcAS16gJBCIkyoyOQ9bGOTtaH9q3vdD7N4w6ILsbwSUMAUpg42Re7IRJQRpPG12S1xiY5rC6ap25nU2r5PqjHYkf6dRUt0CaXGR1n5YGps6VrjFtJENg4TA3ydJe7gDD5VMYktKs6zTHyAFZ8P0slyfOG%2BZoqXMMufa3G3Gv%2F3VPjq1Xjv04L1cONZX1dVHYTTFD07iyxhi1eXzDoINGWptNChks4V1dcAAdzUvtSwiBX7IFIwS82uhT8fkBIF%2F5MyQw1GmTl5OXinaqASX8AdBHGbUU2NJng4J9W0vQDS2WHX%2FqPXvPxEfSlf2TN1ayRvnwGzc%2BAcF7mCfyFb1m23d21OvsYV8gnRU3E7hqzHweQY1ZcuxhAVtWdqtvC3IKh81VUGa%2BMbUiPg2wTl%2B2ZwDyNN2WLsVemVEfOFMnhLCPHZP4AafXDOshebBv8r51KcBp5O%2F%2BYz6ixCO4Yo%2FM9gAmLxJFjCvKQ4FYhYF0lNddE%2BZVq31xC5ijcUjBFWxdHIv8CvUQhhiFcmraJQUGkB29tjEScHKxxgYXeytoDM7p5O5kBHwoUBC4%2BRY5D1WA%2BRb%2BphTg60CWOiIQYchL4kFYeXZUqSUr%2Bd%2BcX8EVB3P9Iah%2F1hCHuKo%2B3ljDMjbXKBjqkAS1BkXOMyA9NALI1BQCuC4VsE9nxkN1v6Tmdp%2Bm1aiRo4pk2EusJJYGgSFh3bAIv0ts8BivPEROB5P1AoICtekVemWdmNeKd5hvCdPTNzAl790Fc8fU5e3rdRSQaKlUoDws%2FXfbOS0KFwtrocTu%2Fu0t6YmxnTohioAkJ5aGMwLXxz0MzyslPzBlGNbEcb4t3rcS4qdyv2YxhhAMMr1oDbl4UB6M8&X-Amz-Signature=0e12587b4696145b74a6b921159206ba4402acc131e5c4d3727a329b67db9e74&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject)

- 서버 기동하면, 소켓 하나 열어서 listen 상태로 전환

- 클라이언트가 connect 호출해서 연결 

- 근데 그 과정에서 connect해서 연결하고 accept 할 때 되면 사이에 일어나는 과정이 3-way handshaking

- 데이터 통신에 필요한 양단의 정보 교환 과정

	- 교환 정보

		- sequence number : 32비트 짜리 부호없는 정수 데이터 → 랜덤 결정됨 → 그 번호를 서버가 결정하면, 누군가 연결하자고 할 때 걔도 sequence number를 보낸다 

		- 이때 보내는 데이터의 단위는 세그먼트

		- 세그먼트에 flag 값 보내는데 SYN을 달아서 보냄 ⇒ 이때 자기 시퀀스 번호 보냄 → 연결 받는 서버는 내가 제대로 받았다는 의미로 클라의 시퀀스 번호에 1을 더해서 보냄과 동시에 내 시퀀스 번호도 알리기 (SYN(4000)+ACK(1001))

		- 이때 연결 되었다고 클라가 판단하고 다시 서버의 시퀀스 번호에 1을 더해서 자기도 보냄 (ACK(4001))

- 네트워크로 보내는데 시간 소요됨 → **핑 = 네트워크 느려, 응답 느리다 → 갔다 돌아오는 시간을 RTT(Rount Trip Time) 만큼의 연결 시간이 기본적으로 걸리고, 함수 실행 연산하면서 시간 더 걸림**

즉, TCP 연결 과정 = **Sequence Number 교환 행위** 

교환되는게 시퀀스 넘버만? X → 시퀀스 넘버, TCP Strategy (관련 정보) 교환 → **MSS(Maximum Segment Size**도 교환됨 , 세그먼트의 최대 크기도 같이 교환하게 되어 있음

<br/>

### 3. 메시지 송/수신


```c++
char szBuffer[128] = { 0 };
while (1)
{
	// 사용자로부터 문자열을 입력받는다
	gets_s(szBuffer);
	if (strcmp(szBuffer, "EXIT") == 0) break; 
	
	// 사용자가 입력한 문자열을 서버에 전송한다
	::send(hSocket, szBuffer, strlen(szBuffer) + 1, 0);
	//서버로부터 방금 보낸 문자열에 대한 에코 메시지를 수신한다
	memset(szBuffer, 0, sizeof(szBuffer));
	::recv(hSocket, szBuffer, sizeof(szBuffer), 0);
	printf("Form server: %s\n", szBuffer);
}
```

- 무한루프

	- 문자열 길이 + 1로 보내 → null 까지 보내려고 

	- 버퍼의 문자열 내용 클리어

	- 서버가 보내준걸 받아서 화면에 출력함

	<br/>

<br/>

### 4. 종료 


```c++
::shutdown(hSocket, SD_BOTH);
::closesocket(hSocket);
```

- shutdown : 클라가 더이상 통신 안하겠다 선언 (입출력 차단) 

- 소켓 닫기 

- 클라는 서버에 비해 단순함 

<br/>

### TCP 연결 종료 과정 (4-way handshaking) 

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/83a69d8f-6be2-4385-96df-7c58b0b2f3bf/5ba91cbc-d4f7-48f0-b5b2-90392e241d37/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB4666ERR373K%2F20251225%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251225T154708Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHYaCXVzLXdlc3QtMiJHMEUCIEDAcZ60vjPW3vAorn3H1Mw5yjy5QSdb%2BhXAbxCtik7PAiEAhrCon4h2phLqK1xBRJU3fSHBibcW31BOvmZAqF2xkDEq%2FwMIPxAAGgw2Mzc0MjMxODM4MDUiDHUwDtN2aem31xhW3yrcAxQvKR3h6Q4xa0kR1n%2Ba%2BwhiEwAyvO3qqERiZqEkFahWQYWP6%2FdU9itqA8qFnngYeqBDLDzZY3uy9BxNg%2FkWOsxZVnNeN1AD%2BV3V28OxInNoV06mT8r6my3x9gPR7N7gULDqAEFWjEhRFIv8oE34oxjpdTfh%2BFBXJ16J%2FYANLVab0BDEY%2BE6kl1I6qHPq4H5DIbyy13Z%2FcCs3bqxx4X11qMn1Y5daA%2F6iUxgYB%2FK%2BHkV7vhOrJougysEPMIKri69bmxkObAxY3%2FE5k3Vef65Uh5YahUrRJsb0bKqoQr78q6AnodEB%2B%2FnPJnkMorUNRN6KTICCt6eMR2O0Oz6NHInRr3J2yp1BxtCuOHNOSl9Is2N0VZ7HawrkVUwRttM%2BTVRdPqfmf7JrAv1CYK5oLBiBGiZAPiEokys7FS5aQ2K6X%2B%2B2mMY12A%2FuP%2Fhkbm6rsNTVYDmxTfqPTjevr8zGXJ4XPobPBloYuHM6r%2BM%2BLvRGuNyQgowKhZE3mpu4%2B61djDaYksxzsPUEcXRvDb02JzS9PsTdiazx89HjisKJ%2Foi1nyQZejVytKDSl46DO%2FZHTq6IkI0GPkcCNS%2FOTRWmWP8e5KFt5b%2BDhQ%2FdPX5kBSCiiHYK3fAURR619rd4KdQMJ6OtcoGOqUBfmJ2xFx%2FCMtanAFIpMEulWBxemrBi8dTP6P9iRbWSHdYoRoi2669oi3KE0rapnn1LenGDqI2il2z80fQG%2BAGL6mTjxg2V%2FHjQDVTCpa%2FSGXRa0xrPTqfQj7%2Br4%2FuZc6QfsojcWMRaQ1M2zAsYmRhFjhQEbYK44IFsmsD4NBBgNAxkwjkxKZQaqNosoBEs8B9jLwLCerzvXHLrpojs3a83KgyS18a&X-Amz-Signature=734504cbb2a73cefec7de5e4f1b63f48239f4cd2a8a33e3ae77f4b48fafd61d6&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject)

- 종료 : 항상 클라가 해야 함 

- 서버가 먼저 종료하면 안됨 

- 클라가 종료하게 유도하자.

- 끊자고 한 쪽에서 나타나는 상태가 **TIME_WAIT**

- 서버에서 TIME_WAIT가 뜨면 안됨

- 서버에 뜨면 = 서버가 먼저 끊자고 했다는 말. 좋은 설계가 아님 

- 연결할 땐 3, 끊을 땐 4 way handshake

<br/>

<br/>

## 에코 클라이언트/서버 테스트 및 Wireshark로 확인

- TCPip 코드 빌드해도 서버 안떠서 왜 안뜨지 했는데 exe 파일 실행하니까 열림 (포트번호 25000)

	- cmd에서 netstat -ano | find “25000” 하니까 발견 

- inet_addr 코드 사용할 수 없다는 에러 

	
```c++
svraddr.sin_addr.S_un.S_addr = inet_addr("127.0.0.1"); // 사용 불가
inet_pton(AF_INET, "127.0.0.1", &svraddr.sin_addr); // 수정 완료 
```

- Server.exe 띄우고, Client.exe 띄움 

	in Wireshark.. 

	![스크린샷_2024-12-02_115357.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/83a69d8f-6be2-4385-96df-7c58b0b2f3bf/bce645ba-c713-445c-84bf-48d366d74cb4/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7_2024-12-02_115357.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB4665FN2VHOZ%2F20251225%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251225T154722Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHYaCXVzLXdlc3QtMiJGMEQCIC0AfE4uSiDaTXef064uM1JRaOz3lqZwvRl9tYizNkdpAiAcXVdhN0pCKMhcpAkWx8ugX1kwnm2XqJGPOcXp2ek2CCr%2FAwg%2FEAAaDDYzNzQyMzE4MzgwNSIMRlQRNblw74m0QNyQKtwD9fBvaXcb53xorEKfAQiChHzs%2FIgRmMUKGvK9dSCAMPXOwSsAMFvfQzzz6d7xE7nz1r3Fmmtt6MFPoHFe%2BAN9QB4BhMVjUe8uuqYTpYa2dtqljAKEszBJCdmTq8%2Fv99GhZj2a4%2FPe4RyPybXLoCnq9XQAfFVgnhtNSjeCOAXfCBT7sOQ8u3g%2FM3QHmMq0C7MJp%2BoxVQY4q9MkmF0oROPPT%2Fyp%2FBujYqtP7j%2F0%2FeP%2BDv50OqlHHGTcntzGXv8fl7QuWhigya8dpI8FzRQbWqLXfiQSne0HqrWgen06yYDnRgNvXxAjCJNL%2BikpgiWzyM5CSRicPk5yPNolQOi0QH5NjijSTuDqCL%2FecjMDN1vMuczQ3HJ%2BSus7Bb6eXlxqAp0ba%2FKgS2E28RM3cJoFd37BElSIOzp9f0GK3M%2BUwGqETePYIQn3IU2tY3DEn8imYdiBGaLq2jWQHEiAJnxBXQbryt5Z3aX5XJ0jJiDggv7uirSdK3tLTsFy2LtW%2BsY3Nlui0SQ0CkDxCKZHxnMPd5NYoIJdaVAF7000xRgkRP4P1a%2B4pDVp9R8yXy3V%2BqubBPxNTJbMt9CiP9OUIp4tnr1HoyRvZfw04ERJLCk9n2LML7XDY5mA0Lw1CGITTJgwxI61ygY6pgEVlyFZxuMvhPMlXc%2BpWwkCBwL3XDS8vFqcQvXSC0jWX6sSAoIholi3Gl2Yw5930lOVvVU7z8MbK6gtQ864wCq68fWuTf%2FdfbBdCXQhQf%2FXHH0Peyi6YaRvoK%2BcO2VPm042gzEdKOCLMA1yzGmT5RCvspxZOfmTaJWx%2FhVpZEOH5zbuXqgUc9PPnF%2B8q7Ty227f71TseZ0hcKRV%2FivsN52NSG%2FBfXTc&X-Amz-Signature=bdc19ece23dedb5fe7f865ebbb3078e1cb1fc89ecfd08f94e3c66a6269bfdf63&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject)

	<br/>

	![스크린샷_2024-12-02_120004.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/83a69d8f-6be2-4385-96df-7c58b0b2f3bf/8fbaa44f-d292-4659-bb3a-b7921516b752/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7_2024-12-02_120004.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB4665FN2VHOZ%2F20251225%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251225T154722Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHYaCXVzLXdlc3QtMiJGMEQCIC0AfE4uSiDaTXef064uM1JRaOz3lqZwvRl9tYizNkdpAiAcXVdhN0pCKMhcpAkWx8ugX1kwnm2XqJGPOcXp2ek2CCr%2FAwg%2FEAAaDDYzNzQyMzE4MzgwNSIMRlQRNblw74m0QNyQKtwD9fBvaXcb53xorEKfAQiChHzs%2FIgRmMUKGvK9dSCAMPXOwSsAMFvfQzzz6d7xE7nz1r3Fmmtt6MFPoHFe%2BAN9QB4BhMVjUe8uuqYTpYa2dtqljAKEszBJCdmTq8%2Fv99GhZj2a4%2FPe4RyPybXLoCnq9XQAfFVgnhtNSjeCOAXfCBT7sOQ8u3g%2FM3QHmMq0C7MJp%2BoxVQY4q9MkmF0oROPPT%2Fyp%2FBujYqtP7j%2F0%2FeP%2BDv50OqlHHGTcntzGXv8fl7QuWhigya8dpI8FzRQbWqLXfiQSne0HqrWgen06yYDnRgNvXxAjCJNL%2BikpgiWzyM5CSRicPk5yPNolQOi0QH5NjijSTuDqCL%2FecjMDN1vMuczQ3HJ%2BSus7Bb6eXlxqAp0ba%2FKgS2E28RM3cJoFd37BElSIOzp9f0GK3M%2BUwGqETePYIQn3IU2tY3DEn8imYdiBGaLq2jWQHEiAJnxBXQbryt5Z3aX5XJ0jJiDggv7uirSdK3tLTsFy2LtW%2BsY3Nlui0SQ0CkDxCKZHxnMPd5NYoIJdaVAF7000xRgkRP4P1a%2B4pDVp9R8yXy3V%2BqubBPxNTJbMt9CiP9OUIp4tnr1HoyRvZfw04ERJLCk9n2LML7XDY5mA0Lw1CGITTJgwxI61ygY6pgEVlyFZxuMvhPMlXc%2BpWwkCBwL3XDS8vFqcQvXSC0jWX6sSAoIholi3Gl2Yw5930lOVvVU7z8MbK6gtQ864wCq68fWuTf%2FdfbBdCXQhQf%2FXHH0Peyi6YaRvoK%2BcO2VPm042gzEdKOCLMA1yzGmT5RCvspxZOfmTaJWx%2FhVpZEOH5zbuXqgUc9PPnF%2B8q7Ty227f71TseZ0hcKRV%2FivsN52NSG%2FBfXTc&X-Amz-Signature=284aeee60074738c0b14bed38564c0f09c930433ade038c0376cae11815f9863&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject)

- 3-way handshaking [SYN], [SYN, ACK], [ACK]

- Data 전송 

- 4-way handshaking [FIN, ACK], [ACK], [FIN,ACK], [ACK]

<br/>

## 소켓 입/출력 버퍼 

소켓 = File

입출력 방법 → 버퍼 사용 

Buffered I/O 

소켓도 마찬가지로 프로세스라는 입출력의 주체가 소켓에 대고 정보를 Send(Write),

소켓에 버퍼가 붙어있음

버퍼가 입력, 출력용 두개로 나뉨 (input output) 

output 버퍼는 write가 일어났을 때 정보를 네트워크로 보내려고 하면 밑으로 내려가야 함 (TCP, IP..) 

<br/>

Hello를 전송한다고 했을 때, 고작 5bit 보내겠다고 1500 짜리 패킷으로 묶어서 보내는게 맞나? 비효율이 심한거 아니야? 

⇒ output 버퍼에 최대한 꽉꽉 채워서 보내면 보내는 패킷의 수는 적게, 패킷 내 데이터는 많게 효율적 전송이 가능한거 아니야? (마치 광역버스가 손님을 꽉 채우기 위해 기다리는 것 처럼) 

**⇒ Nagle 알고리즘** 

	- PPS(초 당 패킷 수)를 줄이자 한번에 보낼 수 있는게 있으면, 버스에 왕창 실어서 한방에 보내자 

	- TCP 수준에 기본적으로 적용됨

	- 데이터 송수신하려고 할 때 소켓에서 어플리케이션으로 전달하려 할 때 버퍼를 최대한 채워서 하자

	- Bufferec I/O의 핵심

	- 본질 : 한번에 많이 보내자 

	- 신호 보내도 그때그때 반응하지 말고 신호 또 왔을 때 한방에 다 태워서 보내면 효율적이잖아

	- 스위칭이 많이 일어나지 않게 해서 효율적으로

	- **Timer 중요**

		- 버퍼가 채워져 ⇒ 쓰기, Read 하든, 버퍼 채워지길 기다려야 해 → 얼마나 기다려? 

		- 표준이 있는게 아니니까 .. 

		- TCP 프로토콜이 구현되는 방식까지 정해주지 않으니까 

	- 얼마나 기다려야할지가 중요한 이슈

	<br/>

**버퍼의 크기**

소켓에 대해 i/o 하는 버퍼 있어 → 크기? **getsockopt()**

- 소켓의 옵션값 확인하는 함수. 짝은 setsockopt()

- 설정 변경, 확인

- 버퍼 값 출력 가능 


```c++
// 소켓의 '송신' 버퍼의 크기를 확인하고 출력
int nBufSize = 0, nLen = sizeof(nBufSize);
if (::getsockopt(hSocket, SOL_SOCKET, SO_SNDBUF, (char*)&nBufSize, &nLen) != SOCKET_ERROR)
	printf("Send buffer size: %d\n", nBufSize);

// 소켓의 '수신' 버퍼의 크기를 확인하고 출력
nBufSize = 0;
nLen = sizeof(nBufSize);
if (::getsockopt(hSocket, SOL_SOCKET, SO_RCVBUF, (char*)&nBufSize, &nLen) != SOCKET_ERROR)
	printf("Receive buffer size: %d\n", nBufSize);
```

- SOL_SOCKET :=소켓수준을 뜻함. 해당 파라미터는 레벨. ⇒  L3? L4?, IP수준? TCP 수준? 

	- 파일수준이라는 의미. 

- SO_RCVBUF : 소켓마다 각각 고유한 버퍼를 따로 갖는다. 소켓 100개 = 버퍼 100개 

- SO_REUSEADDR 도 나중에 다룰 것

<br/>

입출력 버퍼 크기보다 존재 자체를 알아둬야 함

<br/>

I/O Buffer… 

+<span style='color:blue_background'>**send() + recv() 는 과연 1:1로 매핑될까?**</span>

⇒ Echo 시작하고, 채팅서버 만들게 되는게 대부분인데… 보낸거 받는데 1:1로 일치할거라는 착각이 생김

- 128을 보내는데 5로 담아낼 수 있을까? → 여러번 담으면 되잖아. 이 생각을 못함

- 버퍼 크기의 문제 즉, Buffered I/O 하는 것과 1:1 매핑 관념이 잘못 잡히면 어플리케이션 프로토콜 설계가 잘못됨

- **1:1 매핑되지 않는다는 것을 명심**

<br/>

## send()와 recv()는 1:1로 매핑되는가? 

→ <span style='color:red'>**아니다**</span>

send(1); 씩 여러번 했다면 총 3 바이트 전송 → 전체를 recv(128)로 한번에 받을 수 있음

send 간격이 크면 1씩 끊어서 받을 수도 있고, 연속이면 한번에 리턴할 수도 있음

기존 코드 작동 잘 되는데, 만약

- 서버는 여전히 작동 중일 때, 클라에서 옵션을 변경해보자

	- 데이터 보내는데, 클라이언트 입장에서 서버에 데이터 보내는데, 문자열의 길이를 재서 반복문으로 하나씩 send 해보면? 

	- send 여러번 → Hello 보낸다고 하면, 한글자씩 5바이트 send 다섯번 해

	- recv의 반응은? 

		- A를 엄청 많이 보내면? → 중간에 A를 뭉텅이로 한 다섯개 가는 경우도 있음

		- send 여러번 했다고 해도 recv도 1:1로 호출되는게 아님

		- 왜? 

			- 데이터를 write하는데 send 여러번 했다 → output stream에 대한 buffer가 빠르게 반응하면 하나만 쳐도 날아가는데 지연이 생기면 두글자, 세글자… n 글자가 갈 수 있음 nagle 알고리즘 때문에 

			- 버퍼링 했기 때문

			⇒ 서버가 반응하는 것도 요상 

			recv 관념

			- recv의 파라미터로 들어가는 sizeof(szBuffer) ⇒ 128 비트를 읽겠단 시도가 아니라 그럴 수 있으면 읽겠지만, 끊어오겠다는 의미

			- 500, 1000 바이트가 들어오면 128씩 끊겠다

			- 만약 적게 들어오면 적어진 만큼의 일부만이라도 읽어서 반환

			- 128 읽겠다 해도 실제로 읽어오는건 더 적을 수 있다. 

			- **네이글 **알고리즘 때문

- 함수 호출 여러번 해도 네이글 알고리즘으로 인해 묶여갈 수 있다. 

Q. recv 크기 줄이면? 1로 줄이면? 받는 쪽의 크기 줄면? 

<br/>

Q. 버퍼가 있고 없고의 차이? (없을 수 있나?) 

<br/>

## 소켓 입/출력 버퍼와 TCP_NODELAY 옵션 

소켓마다 입출력 버퍼가 따라다닌다

input / output

소켓으로 send할 때, recv 할 때 개입하는 버퍼

Hello같은 문자열을 보낼 때 효율적이야. 하지만 경우에 따라 하나씩 즉시 보내져야할 때가 있다. 

output **버퍼 쓰지마**. 버퍼링 하지마. 무조건 데이터 오면 비효율적이더라도 즉각적으로 반응해줘 

⇒ 게임 

	캐릭터 조작하는 키를 클릭했다. → 한글자 누르고 있어 → 10 바이트도 안되는 입력

	근데 내 캐릭터가 버퍼링하면 캐릭터가 움직이지 않아 → 안돼! 

	게임은 즉각적 반응이 필요하다

	작은 데이터가 대량으로 가면 안되고, 작아도 오버헤드 나고 비효율인거 알아도 빠른 반응성이 중요함

	버퍼링 안하면 됨 

	⇒ **setsockopt**

		
```c++
int nOpt = 1; 
::setsockopt(hSocket, IPPROTO_TCP, <span style='color:red'>**TCP_NODELAY**</span>, (char*)&nOpt, sizeof(nOpt));
```

		소켓 옵션 변경하면 됨 

		**TCP_NODELAY**: 버퍼링 안함

			→ 입출력 버퍼 사용하지 않고 바로 데이터 전송 

			⇒ 클라이언트 코드에서 위의 옵션으로 소켓 수정해주고 테스트하면, A를 엄청 많이 보내도 최대 45를 넘는 패킷이 가지 않는다. 

			그런데 서버에서는 또 버퍼링하고 있는 현상 발생

		<br/>

<br/>

노 딜레이 옵션 사용하면 버퍼링하지 않고 바로 처리 가능

TCP 상에서 데이터 처리 즉각적 → 노딜레이 

노딜레이 옵션을 서버에도 주는게 맞을까? 

→ 그럴 필요 없을 수도..? 

	노딜레이는 클라이언트에서 보통 많이 한다. 

	버퍼링 중요 

<br/>

## 서버 소켓과 SO_REUSEADDR 옵션 (서버만)

소켓의 입출력 버퍼에 관한 옵션은 입출력 전체에 개입하는 부분


```c++
// **바인딩 전에** IP 주소와 포트를 재사용하도록 소켓 옵션을 변경한다
BOOL bOption = TRUE; 
if (::setsockopt(hSocket, SOL_SOCKET, SO_REUSEADDR, (char*)&bOption, sizeof(BOOL)) == SOCKET_ERROR)
{
	puts("ERROR: 소켓 옵션을 변경할 수 없습니다.");
	return 0;
}
```

- 서버에만 특수화된 옵션.

- REUSEADDR : 소켓 수준에 적용하는 옵션

	- 주소(IP)를 재사용하겠다 

	- 소켓은 자원이다. 소켓에 접근하는건 프로세스가 한다.

	- 프로세스가 특히 서버 쪽에서는 bind로 IP, Port를 소켓에 묶는다. 

	- 묶어서 프로세스가 소켓 오픈한다

	- 근데 옆 프로세스가 같은 주소로 오픈하려고 시도한다면? → 안된다는 오류 발생 ← 이미 프로세스 A가 쓰고 있어. 

	- 근데 경우에 따라 오픈이 되어야 할 때가 있다. 

		- 서버는 동작이 항상 Passive 해야 한다. 소켓 닫는거 무조건 클라이언트가 먼저 해야 한다

		- 클라이언트 수준으로 설계해야 하는 이유 = TIME_WAIT 때문 → 연결 하자고 했다가 끊자고 하는 행동을 서버가 하면 서버가 예를 들어 25000 열고 있는데 지가 클라이언트처럼 소켓 닫았어 → TIME_WAIT가 벌어졌는데 close 넘어가야 하는데 얼마나 시간이 걸릴까? → 짧게는 수 초, 길게는 몇 분 

			⇒ 예를 들어, 어떤 프로세스 개발했는데 서버가 닫도록 되어 있는데 잘못 작동해서 죽었는데 TIME_WAIT 걸려서 소켓 회수가 안된다? 

			서버는 재시작하는 것 까지도 생각해야 한다. 안정성을 위해

			안정적 서비스를 위해 재시작 해야 한다면, WAIT 걸려서 소켓 안열리는 문제 생기면 안됨

			다 회수되는데 2분 걸리면 2분 접속이 안돼 → 치명적이야 특히 게임에서는 

			→ REUSE 사용

		서버에서 자의적으로 끊는 경우 이슈 될 수 있음

- 에코 서버에서, 처음 실행한건 잘되는데 똑같은 프로그램 한번 더 하면 에러남 → 이미 바인딩 되어 있으니까. 

- REUSE 옵션 주면 서버 두 개가 동시에 뜸

- 클라이언트가 실행되어서 25000 접속하면 어디에 접속해? 

	- 첫번째 실행한 서버에 연결된다. 

	- 미리 서버가 떠있으면 튕겼다 다시 붙었을 때 나중에 오픈한 서버에 붙는다 

	⇒ 서버 두 개를 띄웠다. 

		클라이언트가 접속하면 1번에 간다. 

		근데 1이 종료되었다면? → 클라이언트 재접속 필요 

		재시작한 클라이언트가 2번에 붙는다 

		하나가 죽으면 다른 하나가 구동 

<br/>

- 내 컴의 IP 주소 = 192.168.0.43 근데 127.0.0.1로 클라이언트가 접속하면? ⇒ 접속 불가 

	- 둘은 다른 주소라고 함. 왜? 어차피 127.0.0.1은 루프백이잖아..? 

	- 나한테 정말로 부여된 실제 주소가 있을 때 둘은 내부적으로 다른 주소로 처리된다. 

	- INADDR_ANY가 아닐 땐 접속하는 클라이언트도 정확하게 주소를 찍어야 한다. 

<br/>

- REUSE 옵션 사용할거면 bind 할 때 서버가 INADDR_ANY 하지 말라고 함 

	- 이유? : 어떤 컴이 있는데, 닉이 여러개일 때, 즉, IP 주소가 여러개일 때 

	- 어떤 프로세스 A, B가 있을 때 둘다 25000 열려고 하는 서버다 → 서버는 INADDR_ANY로 주면, 어느쪽으로 오든 25000만 맞으면 접속 받겠다는게 되어 버림

	- 프로세스가 죽거나 재기동될 때 REUSE 사용한다면, 다른 프로세스가 악의적인 목적을 갖고 … 엉뚱한회선으로 

	- 정보 가로채는 공격 발생할 여지가 있음 

⇒ REUSE 사용할거면 주소를 특정하는게 좋다. 

	<br/>

- REUSE 사용하는 경우 

	- 서버의 안정성 유지 

	- TIME_WAIT 상태로 인해 기다리다가 서버 재시작이 안되는 상황 피하기 위함 

<br/>

## 멀티스레드 에코 서버 

기존 에코 서버 구조 

- 문제

	- 서버가 작동중인데, 클라 1이 접속했다.→ Hello → Hello 다시 옴

	- 새로운 클라 2가 접속하면? 서버가 각각 처리가 가능한가? 접속은 받아주나? 

	- accept 하겠단 루프 도는데, 루프 안에서는 한 클라이언트의 서비스가 반복됨 → 클라 2가 접속되려면 앞서 붙은 클라와의 연결이 끊겨야 accept 된다.

	- 클라이언트들의 처리 완전히 안됨 

	- 서버는 여러 클라의 동시 작동이 가능해야 함

	- 동시성 부여하기 위해 가장 쉬운 방법이 스레드 적용

	- 개별 스레드로 동작하게 하기 

<br/>

**멀티스레드 에코서버**


```c++
// 클라이언트 연결을 받아들이고 새로운 소켓 생성(개방) 
while ((hClient = ::accept(hSocket, (SOCKADDR*)&clientaddr, &nAddrLen)) != INVALID_SOCKET)
{
	// 새 클라이언트와 통신하기 위한 스레드 생성
	// 클라이언트마다 스레드가 하나씩 생성
	hThread = ::CreateThread(
		NULL, // 보안속성 상속
		0, // 스택 메모리는 기본크기
		ThreadFunction, // 스레드로 실행할 함수 이름
		(LPVOID)hClient, // 새로 생성된 클라이언트 소켓
		0, // 생성 플래그는 기본값 사용
		&dwThreadId); // 생성된 스레드ID가 저장될 변수주소 
	
	::CloseHandle(hThread);
}
```

- accpet 루프 도는건 동일

- 연결되면 스레드 바로 생성 

- 클라 소켓 하나를 CreateThread에 넘긴다  

<br/>


```c#
// 연결된 클라이언트와 통신(Echo 서비스) 하기 위한 작업자 스레드 함수
DWORD WINAPI ThreadFunction(LPVOID pParam)
{
	char szBuffer[128] = { 0 };
	int nReceive = 0; 
	SOCKET hClient = (SOCKET)pParam; 
	
	puts("새 클라이언트가 연결되었습니다.");
	
	// 클라이언트로부터 문자열을 수신한다
	while ((nReceive = ::recv(hClient, szBuffer, sizeof(szBuffer), 0)) > 0 )
	{
		// 수신한 문자열을 그대로 반향(Echo) 전송
		::send(hClient, szBuffer, sizeof(szBuffer), 0);
```

- 데이터 오면 그대로 send 

- 작동 방식

	- 서버에 클라이언트1이 접속 → Thread1을 생성 → T1을 통해 서버가 클라1과 통신함

	- 스레드에서 send, recv 모두 하는데, 이건 worker thread이고, 서버의 메인 스레드에서 accept

	- 클라이언트 2가 접속하면 새로운 스레드 생성해서 그걸로 통신함

- 스레드의 개수 = 클라이언트의 개수 

- 테스트

	1. 서버 열기

	1. 클라이언트1 접속하고 Hello 

	1. 클라이언트2 접속하고 World

	⇒ 하나의 서버 창에서 새 클라이언트 연결되었다. Hello → 새 클라이언트 연결되었다. World 출력됨 

		각자 잘 작동

클라이언트 입장에서는 대기 개념이 없음. 멀티 스레딩 하니까 기존에는 C2가 대기했어야 했지만, 이럴 필요 없어짐

서버는 이렇게 작동하는게 맞다. 

<br/>

C1이 연결되어서, Hello 전송했다 .→ 에코잉되어 넘어와 → S가 C1, C2에도 보낸다? → C2가 접속하자마자 World 보냈는데 서버가 C1에도 에코잉 → C1, C2 둘다 자기가 보낸거 안보낸거 다 찍혀 

⇒ **채팅**

→ 서버에서 중계만 잘해주면 채팅이 된다. 에코의 파생 

주변에 클라이언트가 친거 다 알려주면 채팅이지 뭐



	<br/>

## 멀티스레드 채팅 서버 구조와 이론

에코 서버를 확장하면 → 멀티 스레드 기반 에코 ⇒ 확장하면 채팅!

**스레드 개수 = 접속한 클라이언트 수** 

**자료구조** 

- 멀티 스레드면, 동기화가 가장 중요함 

- **동기화** : 임계 구간에 여러 스레드가 접근하는 것 막기 / 스레드 간 시점 맞추기 

**Critical Section, Event**

- 요구되는 이유 = 자료구조 때문

	- 연결리스트 사용할건데, C++의 STL에 있는거 사용 할 것. 

	- 서버는 안정성이 1번. 성능은 2번

	- 자료구조가 등장하게 되어서 동기화 이슈 → 특정 메모리에 대해 잘못된 엑세스 벌어지면 applicatoin error 발생하니까 잘 막아야 함

<br/>

채팅 서버

- 서버 하나와 클라이언트 2개 

- IP, Port로 클라이언트가 서버에 접속해 

- C1, C2 중 하나가 메세지 보내면 전체 클라이언트에 보내주면 채팅이 구현됨 

- IP주소엔 Global, Private 둘이 있는데, Global이 Internet에서 사용하는거 Private은 공유기 

- ipconfig로 알아낸 내 컴 주소를 친구에게 알려줘, 25000으로 접속해줘 

	- IP 주소는 사설 IP 주소이다. 공유기 안쪽 즉, 공유기의 사설 네트워크에서만 통용되는 주소이다. 

	- 사설 주소를 알려주고 접속하라고 하면 인터넷을 통과할 방법이 없다. 

	- 공유기의 설정으로 가서 포트포워드 설정 해야 외부망의 컴이 접속할 수 있음

	- 사설주소 알려주면 접속 못하고, 공유기는 외부 네트워크에 연결되어서 글로벌 ip 배정받게 되어 있음. 공유기에 붙어있는 진짜 ip 주소 알아내서 알려줘야 함

<br/>

<br/>

서버에 대해 외부 클라이언트가 접속한다 

- 접속하자마자 서버에서 스레드를 생성. T1

- 스레드에서 모든 처리 즉, 메세지 전송

- 다른 클라이언트 2 접속 → 스레드 생성 T2

- 둘 중 누가 되었든 메세지 전송하면 소켓, 기타 등 정보 모아서 구조체로 선언해두고. 이 구조체를 **연결리스트**로 만들어서 메세지 누가 보내면 구조체에 전체 엑세스 해서 각 소켓 핸들에 메세지 다 보내면 채팅이 됨

- 큰 문제? : 클라이언트 개수만큼 send를 보내야 함 → 서버의 부하가 기하급수적

- 대부분 채팅 서버는 도배질 하면 급속도로 과부하 발생 → Dos 공격 발생하고 서버 마비됨

- 같은 메세지 보내도 도배질하면 tcp 연결 끊어버리는 방식으로 설계함

- 연결리스트에 줄줄이 있는 클라이언트들, 각 클라이언트들의 소켓 핸들에 메세지를 모두 전송하는 일련의 과정

- **연결 리스트** 전체에 접근하는데 변형이 생기는 시도가 이루어지면 충돌 발생할 것

- 그걸 막기 위해 처음, 끝 지점에 하나의 스레드만 접근해야 함 → 임계구간 설정, 제어 필요 (critical section) 

<br/>

<br/>

## 멀티 스레드 기반 채팅 서버 

### 전역변수


```c++
CRITICAL_SECTION  g_cs;         // 스레드 동기화 객체
SOCKET            g_hSocket;    // 서버의 리슨 소켓
**std::list**<SOCKET> g_listClient; // 연결된 클라이언트 소켓 리스트
```

- socket : 리슨 소켓을 전역으로 뺐다. → 전체 라이프 타임에 개입할 변수라서 

- **연결리스트 **: 소켓을 연결리스트로 관리하겠다. Linked List → 소켓을 연결리스트로 관리함

<br/>

### 클라이언트 관리


```c++
// 새로 연결된 클라이언트의 소켓을 리스트에 저장
BOOL AddUser(SOCKET hSocket)
{
	::EnterCriticalSection(&g_cs); // 임계영역 시작
	// 이 코드는 오직 한 스레드만 수행한다는 것이 보장됨!
	g_listClient.push_back(hSocket); 
	::LeaveCriticalSection(&g_cs); // 임계영역 끝
	
	return TRUE; 
}
```

- 소켓의 핸들을 리스트에 추가하는 함수 

- 임계 구간이 cs로 묶여있다. 

- 스레드가 여러 개일 때 cs 구간에 t1이 진입해서 수행하고 있다면, t2가 진입하고 싶어도 못함, wait (leave 호출될 때 까지 ) 

- 임계구간 → 동시성이 존재하지 않음

- 효율 높이기 위해 멀티 스레드 하는데, 효율을 포기함

- 임계구간은 최소한하는게 좋다. 넓을수록 deadlock 발생 가능성 높고, 효율 낮음

<br/>

### 메세지 전달


```c++
// 연결된 클라이언트 모두에게 메세지를 전송
void SendChattingMessage(char *pszParam)
{ 
	int nLength = strlen(pszParam);
	std::list<SOCKET>::iterator it; 
	
	::EnterCriticalSection(&g_cs); // 임계영역 시작
	// 연결된 모든 클라이언트들에게 같은 메시지를 전달
	for (it = g_listClient.begin(); it != g_listClient.end(); ++it)
			::send(*it, pszParam, sizeof(char)* (nLength +1), 0);
	::LeaveCriticalSection(&g_cs); // 임계영역 끝 
}
```

- pszParam : 보낼 메세지 

- Hello를 메모리로 받음 → iterator를 이용해서 리스트 전체에 엑세스 하게 됨

- 리스트 처음 ~ 끝 순회, 순회를 임계구간으로 설정

- 각각 요소를 가져와서 send 하겠다는 것 

- it = 소켓 핸들

- 1~5를 순차적으로 순회함 → 그럼 뒤에 있는 클라이언트일 수록 메세지 전송에 시간이 더 오래 걸림 (지연 높다) 

- 게임에선 민감할 수 있음 

<br/>

### 서버 종료 


```c++
// 연결된 모든 클라이언트 및 리슨 소켓을 닫고 프로그램을 종료 
::shutdown(g_hSocket, SD_BOTH);

::EnterCriticalSection(&g_cs); // 임계영역 시작
for (it = g_listClient.begin(); it != g_listClient.end(); ++it)
	::closesocket(*it);

// 연결 리스트에 등록된 모든 정보 삭제
g_listClient.clear(); 
::leaveCriticalSection(&g_cs); // 임계영역 끝

puts("모든 클라이언트 연결을 종료했습니다.");
// 클라이언트와 통신하는 스레드들이 종료되길 기다림
::Sleep(100); 
::DeleteCriticalSection(&g_cs);
::closesocket(g_hSocket);
```

서버 종료될 땐, 임계 구간 설정

- 소켓 다 닫고 연결리스트 삭제 

- Sleep(100) : 소켓 닫았는데, 서버가 종료니까 먼저 닫아도 돼… 

	- 클라이언트 다 끊길텐데, 연결된 클라들이 send, recv 하고 있었는데, 소켓이 닫히면 루프 돌고 있었을거 아니야? 

	- 스레드 10개, 100개 모두 종료되는 시점 맞추기 어려워 → 100ms를 기다리면 다 종료되겠지 .. 그러겟거니 하고 대기 

	- 대략 그정도면 스레드 종료되겠지 하고 대기함. 

- 프로세스가 종료되므로, 속해있던 스레드도 운영체제가 다 날림

- 부정확해도 상관없으니까 Sleep으로 사용한 것. 

- 종료를 반드시 감지해야 하는 경우엔 Sleep 사용하면 안된다. 

<br/>

## 처리 스레드 


```c++
puts("새 클라이언트가 연결되었습니다."); 
while((nReceive = ::recv(hClient, szBuffer, sizeof(szBuffer), 0)) > 0)
{
	puts(szBuffer);
	// 수신한 문자열을 연결된 전체 클라이언트들에게 전송
	SendChattingMessage(szBuffer);
	memset(szBuffer, 0, sizeof(szBuffer));
}

puts("클라이언트가 연결을 끊었습니다.");
::EnterCriticalSection(&g_cs); // 임계영역 시작
g_listClient.remove(hClient); 
::LeaveCriticalSection(&g_cs); // 임계영역 끝 
```

- 연결된 클라마다 스레드 생성

- 연결된걸로 데이터 받고, 연결된 클라이언트 전체에 메세지 전송

- 연결된 클라가 여러 갠데, 하나만 끊어졌으면 연결리스트에서 제거 → 삭제니까 임계구간으로 묶어

<br/>

### 연결처리 #1


```c++
// 클라이언트 연결을 받아들이고 새로운 소켓 생성 (개방) 
while ((hClient = ::accept(g_hSocket, (SOCKADDR*)&clientaddr, &nAddrLen)) != INVALID_SOCKET)
{
	if (AddUser(hClient) == FALSE)
	{
		puts("ERROR: 더 이상 클라이언트 연결을 처리할 수 없습니다.");
		CtrlCHandler(CTRL_C_EVENT);
		break; 
	}
```

- accept도 개별 스레드로 분리함

- 콘솔 모드 어플리케이션으로 하는데, 단점이 → 서버는 종료되어야 하는데, … 

	- 콘솔에 핸들러 → CTRL_C_EVENT : Ctrl +c 누르면 이벤트 발생하게 → 프로그램 모두 종료하게 하기 

	- 전체 클라이언트 다 잘리게 

	<br/>


```c++
// 클라이언트로부터 문자열을 수신함
hThread = ::CreateThread(
					NULL, // 보안속성 상속
					0, // 스택 메모리는 기본 크기
					ThreadFunction, // 스레드로 실행할 함수 이름
					(LPVOID)hClient, // 새로 생성된 클라이언트 소켓	
					0, // 생성 플래그는 기본값 사용
					&dwThreadID); // 생성된 스레드 ID가 저장될 변수 주소 
::CloseHandle(hThread);
```

- 스레드 생성해서 멀티스레드로 … 

<br/>

[ 코드 설명들 ]

- ::SetConsoleCtrlHandler : CtrlHandler (함수, ctrl+c 이벤트 감지해 프로그램 종료) 

	- 에코 서버 때는 ctrl+c 해도 서버 종료 안되었었음 → 위 처럼 구현해주면 ctrl+c 이벤트를 감지함 

<br/>

## 멀티스레드 기반 채팅 클라이언트 

- 기존과 차이 

	- 에코는 남의 메세지 볼 일이 없어서 처리의 흐름이 하나였지만, 채팅은 멀티스레드 

	- 사용자 입력, 송신 처리는 메인 스레드에서 

	- 서버가 보내주는 채팅 메세지 수신 및 처리 전담 스레드 분리 (recv 언제가 될지 모르니까) 

		- 언제 보낼지 모르니까 메세지 전달은 나의 입장에서 비동기적 이벤트 → 작업 스레드 분리해서 수신 따로. GUI 락 안걸리게 

<br/>

### 수신 #1


```c++
// 채팅 메세지 **수신 **스레드 생성
DWORD dwThreadID = 0;
HANDLE hThread = ::CreateThread(NULL, 
																0, 
																ThreadReceive, 
																(LPVOID)hSocket, 
																0, 
																&dwThreadID);
::CloseHandle(hThread);	
```

- 사용자 키 입력받아서 보내는 것만 메인 스레드에서.

- 보내는건 ThreadReceive

<br/>

### 수신 #2


```c++
// 서버가 보낸 메시지를 수신하고 화면에 출력하는 스레드 함수
DWORD WINAPI THreadReceive(LPVOID pParam)
{
	SOCKET hSocket = (SOCKET)pParam; 
	char szBuffer[128] = { 0 };
	while (::recv(hSocket, szBuffer, sizeof(szBuffer), 0) > 0)
	{
		printf("-> %s\n", szBuffer);
		memset(szBuffer, 0, sizeof(szBuffer));
	}
	puts("수신 스레드가 끝났습니다.");
	return 0;
}
```

- recv만 함

- 메세지 오면 화면에 찍고 루프 돌기 

<br/>

### 종료처리


```c++
// 소켓을 닫고 종료
:;closesocket(hSocket);
// 스레드가 종료될 수 있도록 잠시 대기
::Sleep(100);
// 윈속해제
::WSACleanup();
return 0;
```

- 수신 #2의 while에서 recv 함수가 0을 반환할 때, 즉, hSocket이 close될 때 자동 0 반환 → 스레드 종료 

- hSocket을 닫으면 작업자 스레드에서 recv가 0 반환함→ 종료될 때 까지 조금 기다렸다가 종료하게 

<br/>

## 우아하지 않은 비정상 종료 

- 종료  : TCP 연결의 종료 

- TCP 연결의 종료 이유 : 소켓을 닫았기 때문 

- 비정상? : 소켓의 강제 회수 (운영체제 수준에서) 

	- 소켓을 개방한 프로세스가 죽었다는 의미 ⇒ 강제회수 발생

	- 프로세스가 비정상 종료

- TCP 상태의 RST : Reset → 연결 끊어짐 

	- 4-way가 아니라 연결종료 

- 프로세스 종료되면 OS가 할당한 자원 강제회수, RST 발생하는데

- 더 심각한 경우? 

	- **LAN선 분리** : 서버가 라우터 타고 넘어가서 인터넷 망 지나 다른 클라이언트와 연결되어 있을 ㄹ때 

	- 서버는 한국에 클라는 미국에 

	- 미국에 있는 컴퓨터가 프로세스 강제로 죽었으면 RST가 온다. 서버가 쟤가 종료되었음 알 수 있어

	- 근데 랜케이블 그냥 분리했으면 쟤가 끊긴거 알 수 없다 

	- 아니면 블루스크린 발생 (강력한 에러) → 아무런 신호도 안옴 → 서버는 아직도 연결되어 있단 착각

	- TCP 연결이 정상적이지 않고, 서버에서 사라지지도 않은 상태 

	- TCP 연결 상태 = 좀비세션이라 함

	- 중간에서 랜 케이블 뽑고 이런 경우 외에도 … 네트워크가 불안정, 장치이상으로 인해서 좀비 발생하기도 함

- 대체 방법? 

	- 하트비트 (연결을 절대로 신뢰하면 안된다.) → Hearbeat를 계속 보내야 함

	- 서버 입장에선 주기적으로 뭘 보낸다. → 살아있니? → 응답 안보내면 죽었나봐

	- 연결 좀비세션으로 간주하고 삭제 함. 

	- 세션 믿지 마세요 

	- 연결은 착각이야 

	- 리얼타임으로 인지할 방법 없어 

	- 클라이언트 중엔 악의 가진 사람 (해킹이든 공격이든) → 온갖 짓을 다해 → 악의적 행동이 무엇이 일어나더라도 문제 발생하지 않게 해야 함 

- 하트비트 구현 

	- 채팅에 … 

	- 클라이언트의 대응, … 

<br/>

<br/>

## TCP 채팅 서버 - 성능 개선

- I/O 멀티플렉싱 서버 

	- 통신하기 위한 채널 있고, 멀티플렉싱 = 거기에 서브로 하나의 채널 안에 입출력 요청이 동시에 공존 가능

	- **유저 모드 어플리케이션 프로세스가 입출력 주도하긴 하는데, 실제로 입출력을 누가해? = OS → 프로세스는 입출력 해달라고 요청하는 구조로 개발하게 되어 있음**

		→ 파일 오픈해 ⇒ fwrite() → 파일에 쓰기 발생 ⇒ 썼다면, 파일에 대고 정보 전송한 것 = 이걸 갖고 파일에 대한 쓰기 OS에 써달라고 프로세스가 요청한 것.

		정말로 쓴건 운영체제

		→ <span style='color:blue_background'>**모든 입/출력은 프로세스가 아니라 OS가 주도**</span>

	→ 멀티플렉싱 = OS가 이미 해 줌

		서버 개발하는 우리가 멀티플렉싱이라는걸 경험할 땐, 현상만 경험함. 베이스에서 일어나는건 모두 운영체제

		**송신과 수신이 동시발생**

		- send 하고 있는데 동시에 recv 가능? → 멀티스레드로 돌리면 가능한거 아니야? → 하드로 가능하면 소프트로도 가능함

<br/>

**동기/비동기**

- 프로세스 입장에서 동기/비동기 

### 동기

- 파일에 대고 프로세스가 write → 써달라는 요청이 운영체제에 전달 →NTFS(File System) → Driver → 실제로 디스크에 쓰고 → Driver → NTFS →  함수 리턴 ⇒ **동기** 

	- write가 끝날 때 까지 콜한 함수/스레드가 반환될 때 까지 wait → 동기

### 비동기

- 커널에 Queue가 있음 → write가 비동기면, 운영체제에서 write 콜하면 A를 메모리에 복사해서 OS가 들고 있다. 이걸 큐에 집어넣고 함수 자체가 밑단(Driver, H/W)으로 가지 않고 바로 리턴함

	- 끝난건 아님. 큐에 추가하고 리턴한거니까 

	- 실제 write는 큐에 있는거 퍼다가 일해야 함

	- 이런 처리를 큐에 추가하는 것만 하고 리턴함 

	- 입출력이 Pending 되었다 함

	- 펜딩되었으니 실제 완료 시점은 몰라. OS가 알아서 해 

	- 정말 어딘가에 완료 되었다면, 운영체제가 그걸 유저모드 어플리케이션에 어떻게 알려? 

		- 함수 콜백하거나, 이벤트같은 시그널을 set 해 

		- 실제 끝난 시점을 알고 싶으면 콜백, 이벤트 사용해 

<br/>

→ 네크워크에서의 비동기? 

- 서버가 리슨 소켓 하나 열어서 통신할 수 있는데, 클라이언트가 늘어나면 소켓이 늘어나겠지 

- 서버는 세 클라 소켓에 대해 스레드를 다 만들었다면, 스레드가 존재하는 순간, 스레드는 cpu  자원 소모함. 근데 A 스레드 연산하다 B 연산하는 컨텍스트 스위칭 일어나면 비용 발생함 → 스레드는 적게 만들자.

- 여러 개 둔다면, 최소화해야 함. 스위칭을 최소화 (Context Switching)

- 비동기 입출력 하다보면 멀티 플렉싱 자동 발생함

	- write, write, write 걸면 다 큐에 다 펜딩 → Os가 알아서 처리해 → 펜딩해서 거는덴 시간 얼마 안걸려 → 싱글 스레딩 해도 상관없어

	- 근데 소켓에서는 차라리 파일 IO는 내 PC의 하드니까 그나마 괜찮은데, Remote의 PC라고 한다면, 메세지들은 언제 올지 모르잖아

	- 상대방이 타자를 치는 시점… 네트워크 입출력의 본질은 모두 비동기

	- 네트워크로 뭔가 왔다면 그건 OS가 알고 있다. → OS가 니가 수신할 데이터가 있다고 마킹하는 방식으로 알려준다면, 싱글 스레딩 상황에서도 비동기적인 입출력 request가 언제 있었는지 알 수 있음 → MultiPlexing + Select 

	- select :  변화 감시 

		- 실질적인 입출력 주체는 Os이다 보니, OS가 변화를 알려줬을 때 그 변화를 인지해서 변화가 발생한 애를 콕 찝어서 I/O 해주는게 select 

<br/>

### I/O 멀티플렉싱 서버 - 변화 감시 


```c++
// 5. 소켓의 변화 감시를 위한 반복문
<span style='color:red'>**UNIT nCount; 
FD_SET fdRead; **</span>
std::list<SOCKET>::iterator it; 

puts("I/O 멀티플렉싱 채팅 서버를 시작합니다.");
do
{
	// 5-1. 클라이언트 접속 및 정보 수신 변화 감시셋 초기화 
	FD_ZERO(&fdRead);
	for(it = g_listClient.begin(); it != g_listClient.end(); ++it)
		FD_SET(*it, &fdRead);
		
	// 5-2. 변화가 발생할 때 까지 대기
	::select(0, &fdRead, NULL, NULL, NULL);
```

- FD_SET : Fd=File Descripter SET (배열) 

	- 64개 밖에 안됨 → 배열 최대 크기 

	- 소켓의 배열 

- FD_ZERO : 배열 초기화 

- for : SET 최기화 

- 0으로 배열 초기화 한 다음에 클라이언트 연결 되었으면 연결된 소켓에 대해 변화가 있는지 조사함

- select : 변화 생길 때 까지 기다려

	- 변화 = 누군가 기록해줘야 함 = OS 

<br/>


```c++
// 5-3. 변화가 감지된 소켓 셋 확인
nCount = fdRead.fd_count; 
for(int nIndex = 0; nIndex < nCount; ++nIndex)
{
	// 소켓에 변화 감시 플래그가 세트 되었는가? 
	if (!FD_ISSET(fdRead.fd_array[nIndex], &fdRead)) continue; 
```

- send/recv/connect/listen/accept 등 변화… 

<br/>

**연결 감지**


```c++
// 5-3-1. 서버의 listen 소켓이 세트되었는가? 
// 즉, 누군가 연결을 시도했는가? 
if (fdRead.fd_array[nIndex] == g_hSocket)
{
	// 새 클라이언트의 접속을 받는다
	SOCKADDR_IN clientaddr = { 0 };
	int nAddrLen = sizeof(clientaddr);
	SOCKET hClient = ::accept(g_hSocket, (SOCKADDR*)&clientaddr, &nAddrLen); 
	if (hClient != INVALID_SOCKET)
	{
		FD_SET(hClient, &fdRead);
		g_listClient.push_back(hClient);
	}
}
```

<br/>

**연결 종료 감지**


```c++
// 5-3-2. 클라이언트가 전송한 데이터가 있는 경우 
else
{
	char szBuffer[1024] = { 0 };
	int nReceive = ::recv(fdRead.fd_array[nIndex], (char*)szBuffer, sizeof(szBuffer), 0);
	if (nReceive <= 0)
	{
		// 연결 종료 
		::closesocket(fdRead.fd_array[nIndex]);
		FD_CLR(fdRead.fd_array[nIndex], &fdRead);
		g_listClient.remove(fdRead.fd_array[nIndex]);
		puts("클라이언트가 연결을 끊었습니다.");
	}
```

<br/>

<br/>

**수신 감지**


```c++
	else
	{
		// 채팅 메세지 전송
		SendMessageAll(szBuffer, nReceive);
	}
```

<br/>

- 소켓 핸들 배열 (최대 64)

	- 데이터 날라가든, 오든, 닫든 할텐데

	- 변화가 생겼으면 어떤 변화인지 내가 알아내서 작동하는 것

	- 변화 발생은 OS가 함

- 변화 발생할 때 까지 select에서 대기 (멈춤)

	- 배열을 만들어놓고, 나 이거 감시할거라고 준비해두고 OS에게 변화 발생하면 알려달라고 select 호출

	- 운영체제에서 변화 생기면 마킹할텐데, 변화 생기면 리턴

	- 변화 생기면 루프 돌자 

	- 그 변화가 뭔지 루프에서 찾아내 

	- 들어온 변화 종류에 따른 처리 수행 

- nReceive가 0이면 연결 종료된거니까 연결 닫아주고

- 그게 아니면 채팅 전송

- 근데 코드에서 스레드 생성 안함 = 싱글 스레딩

	- 그럼에도 불구하고 멀티 스레드 채팅처럼 동일하게 작동함

	- event select는 동시 감시가 64개 까지 가능

	- select 방식이면, 기다렸다가 os가 select에서 대기하다가 날라오면 대응함 (핵심) 

<br/>

- 요즘 select 안쓴다는데? 

	- i/o multiplexing 은 이론 공부… 실제로 실무에서 만질 일이 없다는데요

	- 더 좋은게 있으니까. . .   . ..  

	- 좋은점은 싱글스레딩, 감지는 OS에게 도움 → 스레드가 하나임에도 비동기적 처리가 가능한 것 처럼 보이고, 싱글 스레드라서 스위칭 없다는 효율적 장점

<br/>

[ 발전 방향 ] 

- 처음엔 하나의 클라이언트만 처리하다가 멀티 스레드로 여러 클라이언트를 동시에 하다가 거기서 오는 비효율을 해결하기 위해 멀티플렉싱이 나타난 것

- 궁극은 IOCP인데, 지금의 과정은 발전 과정인 것. 

<br/>

**event select**

- 이벤트를 감지하는 방식

- 구조가 조금 더 깔끔함

- 그냥 select는 FD_SET (배열, 64) → 소켓 핸들의 배열, OS에 던져주면 OS가 변화 감지하고 알림, select가 변화 알림 

	- 서버 소켓도 묶으니까 모든 입출력 처리를 싱글 스레드로 가능했다. (스위칭 없어서 효율) 

	- 모든 입출력은 OS

- Event select

	- WSAEVENT : 64개

	- 기존에 단순 select와 큰 차이는 아니지만

	- OVERLAPPED

	- WSAEVENT = HANDLE 

	- 기존엔 소켓 있으면 소켓 객체가 최대 64, 단 한개만이라도 입출력 요구 있으면 select 함수 반환해서 처리함

	- event selec t; 소켓이 하나씩 있었다면, 각각에 대해 이벤트 객체 같이 존재함 → 뭐가 반응했는지 체크 → 루프 돌 필요없다. → 64개 중 어떤게 반응했는지 루프로 찾을 필요 없음

	- 소켓, 이벤트 어레이를 전달해서 뭔 일 나면 알려달라 

<br/>

**핵심 코드**


```c++
// 5. 루프를 돌면서 이벤트를 처리한다
DWORD dwIndex;
WSANETWORKEVENTS netEvent; 
while (TRUE)
{
	// 소켓 이벤트를 기다린다
	dwIndex = ::WSAWaitForMultipleEvents(
								g_nListIndex + 1, // 감시할 이벤트 개수
								g_aListEvent,     // 이벤트 배열
								FALSE,            // 전체에 대해 대기하지는 않는다
								100,              // 100ms 동안 대기
								FALSE);           // 호출자 스레드 상태를 변경하지 않음
	if (dwIndex == WSA_WAIT_FAILED) continue;
```

- set : waitforsingle object 함수 리턴 

- 여러 개 감시할 땐 WSAWaitForMultipleEvent : 하나라도 이벤트 오면 반환함

	- 그 중 뭐가 반환했냐?  WSAEnumNetworkEvents로 이벤트가 발생한 소켓의 인덱스 및 이벤트 발생 이유 확인 

<br/>

→ 스레드 없어 

	다만, 이벤트 셀렉트 방식이 코드는 좀 깔끔한데, 윈도우 밖에 못쓴다. 

	소켓과 핸들을 매핑해서 이벤트 셀렉트를 하겠다 → 루프 돌기 

	배열에 세트 된 어떤게 이벤트가 왔다, 입출력 필요성이 생겼으면 WSAWaitForMultipleEvents 함수 반환

	어떤게 어떻게 됐다는 거지? WSAEnumNetworkEvents

	멀티 스레딩과 같은 효과가 난다 ( context switching이 없기 때문에 효율적)

	**비동기적 처리의 핵심엔 운영체제의 알림이 있다. **

	모든 입출력은 os가 함

	os가 제공하는 서비스 잘 알아두고 잘 써먹어야함

<br/>

## 파일 송/수신

### 기본 구조에 대한 설명

- 기존 

	- 소켓을 통해서 char[](문자열), 작은 길이를 배열에 담아서 송수신했었다. 

	- Echo → Chatting … 

- 파일

	- 조금 커진 규모

	- 데이터양 : 데이터 양 확 늘어남

	- 문자열은 100바이트도 제대로 안됐었는데 파일은 많자면 GB도 있지 

	- 용량이 늘어나니까 생각해야 할 문제 생기고, 양이 늘어난다 = 송수신 소요시간 증가 

	- 대역폭 소진도 커짐

	- 파일을 서버든, 클라든 하나는 보내고, 하나는 받는 형식

	- 클라가 서버에 요청 전송 : 파일 보내줘 (Request 전송) 

	- 서버가 그래 알았어 하고 파일 보내줌 

	- 가장 심플한 예제… / 메세지는 짧으니까 그냥 보내면 되는거였는데, 파일은 크기가 커지니까 (MB~GB) 버퍼 사이즈(IO) , 한방에 메모리에 파일을 올릴 수 있을까? → 불가능하진 않지만 그렇게 개발하지 않음

	- 1GB를 메모리에 다 올리는거 = 메리트 없어

<br/>

**웹 서비스 기본 구조** 

- 웹 클라이언트 = 브라우저 

	- 역할 : 서버에 TCP 연결, 문서 전송요청(HTML), 문서를 수신

	- 파일에 대한 Request이고, 서버가 Response 보냄

	- 파일 송수신 = 성능 생각 + 장애 고려해야 함

<br/>

클라이언트 - 서버

- 서버에서 파일 송신한다면, 소켓 인터페이스 있을 것. 

- TCP/IP 연결 되면, TCP 세션 성립되면, 통신 위한 소켓이 서버에 생기겠지

- 파일 읽어와서 메모리에 복사해야 함 → Read 먼저 해야 함 

- 파일로부터 데이터 읽어와서 프로세스의 메모리에 카피해야 함

- 파일은 1GB 더라도, 메모리 확보가 되어야 하고, 통신 소켓에 send 해야 함

- 근데 I/O 버퍼 또 있지 → 또 복사

- 네트워크 타고 내려갈 때는 패킷으로 만듦

- 클라이언트에서도 어딘가 메모리에 담겠지 

- TCP 자체가 서버에서 송신하다 장애 날 때 수신이 원인일 수도 있음

	→ 데이터 오면 TCP 메모리에 차곡차곡 하는데 소켓에 쌓아올리는 쪽은 OS가 하는데, 프로세스 메모리에서 빨리 퍼가야 함

	recv로 빨리 퍼가야하는데, 읽기 속도가 수신 속도가 낮으면 버퍼에 계속 쌓이겠지 

	파일송수신은 수신 측에서 운영체제의 역할 지점을 제외한 어플리케이션에서 어떻게든 퍼올려가는 구조를 안만들면 장애 발생함

- 읽기 속도 높이기 위한 성능 극대화 방안 고려해야 함

<br/>

[ 순서 ]

1. 서버의 HW에서 파일 가져오기 read()

1. 서버 측 프로세스(어플리케이션)의 메모리에 가져오기 copy

1. 서버의 통신 소켓의 버퍼에 복사, send()

1. 클라 측 IP 버퍼 → 클라 측 TCP 버퍼

1. 클라 측 소켓의 버퍼

1. 클라 측 프로세스(어플리케이션)의 메모리에 recv() 

<br/>

### 파일 송신 서버 제작 

**프로토콜이 없는 파일 송신**


```c++
puts("클라이언트가 연결되었습니다.");

// 파일 송신
char byBuffer[65536]; // 64KB
int nRead, nSent, i = 0;
while ((nRead = fread(byBuffer, sizeof(char), 65536, fp)) > 0)
{
	// 파일에서 읽고 소켓으로 전송한다.
	// 전송에 성공하더라도 nRead와 nSent 값은 다를 수 있다!!!
	nSent = send(hClient, byBuffer, nRead, 0);
	printf("[%04d] 전송된 데이터 크기: %d\n", ++i, nSent);
	fflush(stdout);
}
```

- 서버에 클라이언트가 접속하면, 서버에서 파일을 클라 측으로 송신하는 구조 

- 프로토콜이 없다 = 요청 보낸다고 하면, 응답하고, 파일만 보내는 구조라서

- 소켓 단위 송수신 버퍼 사이즈도 64KB

- fread 함수 → 파일을 바이너리로 열었다는 의미

	- 읽어서 64K 만큼 읽으려고 함

	- 데이터 읽었으면 (>0) 네트워크에 대고 전송하겠다

	- 전송된다 → 내가 보내려고 시도한 nRead가 실제로 보낸 데이터 nSent와 동일하지 않을 수 있음

	- 네트워크로 읽은만큼 보낸게 갔는지 확인해야 함 → 예외처리, 에러처리.. 필요하긴 함

- 다 안갔다고 보내는 쪽 잘못만이 아님. 수신 측이 32만 받았을 수도 있어. 여유가 없어서

	- 보낸거 ≠ 받은거 일 수 있음

	- 항상 nSent가 작을 수 있음 → 이때, 수신 측에서 여유 없어서 수신 불가한 상황이구나 판단하자. 

	- 아님 서버의 네트워크 이상일 수도 있지만.. 

	- 100 바이트 보냈는데, 50바이트만 갔으면 50은 못보냈어 → 수신 측 여유 없음 문제

<br/>

- 파일을 보내겠다는 서버 측은 메모리를 64KB 확보 → 파일로부터 64 읽어서 메모리 꽉 채워

- 클라와의 통신 소켓에 대고 send() 함. (write) 

- TCP →IP 로 내려가서 전송

- 소켓의 Output 버퍼가 있을텐데, 64KB → 꽉 차서 내려가니까 효율적…

- 64로 꼭 해야하나요? → 정답은 아님. 제일 좋은건 버퍼 크기 많이 잡을 수록 좋다. 함수 호출 덜하고 송신을 적게할 수 있으니까. 

	- 한번에 많이 읽어서 보낼 수 있으면 그게 좋겠지

	- 이게 튜닝의 대상임.

- while 루프 : 파일의 시작부터 끝까지 64 단위로 가져와서 send → 몇 번 돌지 용량에 따라 다르겠지

	- 반복문 덜 돌면 좋지 → 그럼 메모리 많이 써야 함

<br/>

### 파일 수신 클라이언트 제작

- 소켓 생성

- 서버 통신하는 포트 25000 사용

- 접속 시 루프백 주소 사용

- 파일을 서버에 연결하면 요청하지도 않고 무턱대고 시작하고 파일 읽음

- 파일명 정해져있음 → 보낼 파일, 수신할 파일 정해져있음 → 윈도우에 들어있던 음악파일

- 압축으로 되어 있는데 → 압축은 송수신 간 손상되면 체크하는데 손상되었는지 알려줌

- 압축 파일로 테스트해서 받은 압축 파일이 잘 풀리면 안깨진 것 

- MD5 해시값? 해시 결과 일치하면 손상 없구나 판단

- recv : 소켓 버퍼에 데이터 온걸 카피해서 byBuffer로 퍼가겠다 

- recv 속도가 네트워크 전송 속도보다 무조건 빨라야 함 → 읽었는데, write 하는 과정 → 수신 속도 > 쓰는 속도 

- 고속으로 유입되는 데이터를 손실없이 저장한다 … 

- 디스크 쓰기는 대단히 느린 오퍼레이션

- 데이터 메모리 확보 동적할당하고, 데이터 쓰기, 처리를 별도 쓰레드로 빼고, 큐 만들어서 달아서 집어넣는 구조로 만들고, 느리더라도 처리 가능하게 하는게 좋은 설계 

- 파일 크기 얼만지도 모르고 받음 → 64 미만 데이터 날라오면 마지막 조각이라고 봐도 됨? → 안됨 ⇒ 장애 있었을 수도 있잖아 

	- 적어도, 서버에서 뭔가 보내기 전에 사전에 주고받아야 하는게..  파일명, 파일 크기 

	- 정교하게 하고싶으면 MD5 해시값도 같이 받았으면 좋음

<br/>

cf. 파일을 보낸다. → 송신 행위는 아주 잦다. → 함수 없어? → 다 있어 ⇒ 송신은 이미 되어 있음. 프로토콜을 디자인해서 넣어보는… 

<br/>

### win32 API 기반 파일 송/수신 

**파일 송신 전용 - API**


```c++
// 파일송신
if (::TransmitFile(
				hClient, // 파일을 전송할 소켓 핸들
				hFile,   // 전송할 파일 핸들
				0,       // 전송할 크기, 0이면 전체 -> hFile의 크기를 알아서 산정 
				65536,   // 한번에 전송할 버퍼 크기
				NULL,    // 비동기 입/출력에 대한 OVERLAPPED 구조체
				&tfb,    // 파일 전송에 앞서 먼저 전송할 데이터
				0        // 기타 옵션
				) == FALSE)
		ErrorHandler("파일을 전송할 수 없습니다.");

// 클라이언트가 연결을 끊기를 대기
::recv(hClient, NULL, 0, 0);
puts("클라이언트가 연결을 끊었습니다.");
```

- 파일 송신하는 전용 API 이미 있음

- **이 함수만 잘 쓰기**

- <span style='color:red'>**NULL, &tfb**</span>

	- 비동기 지원

	- 먼저 전송할 데이터 → 구조체(파일 명, 크기) 

	- 파일 데이터 받기 전에 받으니까 연결 1번, 메타데이터를 2번으로 수신함

	- 이걸로 파일 생성하면 되는 것. 

	- 소켓 안에 정보 (파일 명) 있으니까 이걸로 파일 생성함

	- 그 후에 나머지 수신은 동일

	- 프로토콜이 적용된 결과임. 

- 서버에 연결하면, 서버에게 파일 달라는게 아니라 일방적 보내기 상태긴 함. (수정필요) 서버 측에선 파일에 대한 메타데이터, 파일 데이터를 구분해서 send 각자 함.

<br/>

**응용프로그램 프로토콜 디자인 - 수신**


```c++
if (::connect(hSocket, (SOCKADDR*)&svraddr, sizeof(svraddr)) == SOCKET_ERROR)
	ErrorHandler("서버 연결 불가");

// 수신할 파일명, 크기 정보 먼저 받기
MY_FILE_DATA fData = { 0 };
if (::recv(hSocket, (char*)&fData, sizeof(fData), 0) < sizeof(fData))
	ErrorHandler("파일 정보 수신 못함");
```

<br/>

- 파일에 관한 정보 함께 보냄으로써 프로그램의 정교화, 유연성 증가 

- 구조체에 MD5 해시 데이터도 넣어보면 좋음 (원본에 대한 해시)

<br/>

Q. 서버가 연결을 끊는건 안좋아! 클라이언트가 끊게 해야 하는데.. 어떻게 구현하지? 

A. 클라이언트가 연결을 끊기를 대기함

	
```c++
::recv(hClient, NULL, 0, 0);
```

	받는 쪽에서 recv 함수로 0을 리턴받으면 close socket 

<br/>

cf. TransmitFile엔 파일 핸들 보내야 하는데, fopen 하면 핸들 안생기기 때문에 ::CreateFile()을 사용해야 함 

	
```c++
// 전송할 파일 개방
HANDLE hFile = ::CreateFile(_T("Sleep Away.zip"),
		GENERIC_READ, FILE_SHARE_READ, NULL ...);
```

<br/>

<br/>

### 대표적인 TCP 장애 유형과 연결에 대한 정의 

< 대표적인 TCP 장애 유형 >

- Packet Loss : 패킷이 유실된 경우

	- 인터넷 (패킷 교환망) → 패킷의 최대 크기는 MTU, 약 1500 바이트) → 인터넷 네트워크는 안정적이지 않다.

	- L3 수준 혹은 이하에서 발생하는 흔한 문제가 패킷 유실 

	- 패킷 = 택배… 배송되다가 물건이 사라졌다  

- TCp Out of order : 패킷의 순서가 뒤바뀐 경우

	- connection oriented (연결) .. 

	- 순서를 따지는데, 가끔 뒤바뀐다. 경우에 따라 재전송 이루어질 수도 있음

- Retransmission과 Dup ACK 

	- 재전송, duplicate ack 

	- 수신한 측에서 ACK 보내게 되어있는데 순서가 바뀐 경우 → 2가 와야 하는데 3이 왔네? → 2 다시 보내줘! → 근데 조금 있다가 2가 옴 → 잘 받았어! -.. ACK의 중복

- **Zero-window** : 수신측 버퍼에 여유 공간이 하나도 없는 경우 

	- L1~2 : HW ~ Driver

	- L3 : IP

	- L4  = TCP 

	- L7 : HTTP.. 

	- L5 : SSL .. 

	- 소켓 프로그래밍은 L5 이상 

	- 패킷 유실은 보통 L1~L2, 재전송은 L4(TCP) → 운영체제 수준, 혹은 하드웨어라서 개발자가 거기까지 고민할 필요는 없.. 

	- 소켓도 버퍼 갖고 있고, TCP도 운영체제 수준의 버퍼 갖고 있음. 프로세스에서도 버퍼 갖고 있음

	- 패킷 단위로 날라와서 유입되면 TCP 버퍼에 차곡차곡 쌓음

	- 소켓 수준에서 쌓이는데, 남은 공간 크기가 0이 되면 = zero window

	- zero 이면, recv 함수로 소켓에 쌓여가는 데이터를 내가 처리하기 위한 버퍼로 가져오는데 (move) → 내가 빨리 퍼갈수록 여유공간은 늘어남

	- 네트워크에서 쌓아올리는 수신 속도 내가 이동시켜서 처리하는 처리속도를 평상시로 유지해야 함

	- 수신 측에서 zero에 가까워지면 송신 측에 알려줌 

	<br/>

L = Layer

- TCP는 L4

	- IP는 L3 

- 패킷 유실, 순서 변경, 재전송은 네트워크 인프라 수준의 발생 오류임

<br/>

<br/>

TCP는 연결과 필연, 

- 서버 개발자 관점에서 클라이언트 연결은 어떻게 정의? 

- <span style='color:red'>**연결 해제는 어떻게 알 수 있는가? **</span>

	- 서버, 클라 사이 인터넷이라는 공간 있다. 서버 입장에서 Remote인 클라이언트에게 어떤 일이 일어나는지 알 수 없다

	- 네트워크는 연결 자체가 주관적 판단에 기초함

- 클라이언트가 LAN 케이블을 분리하거나, 시스템 Reset 혹은 정전 상황을 감지할 수 있는가? 

	- 연결의 사실상 해제 

	- 연결은 물리전 수준에서 끊어졌을텐데, TCP는 해봐야 4계층 → 논리적 연결 유지된다고 말할 수 없음

	- 소켓 프로그래밍의 관점에서, 인지할 방법 없다

- 연결이 되었다 = 서버 입장에서, accept 함수가 리턴했다. 클라는 connect 함수가 리턴했따.

	- 그 후, send/recv 간에도 연결은 제대로 유지되고 있느냐? → 클라이언트는 언제 어느 때 날라가버렸을지 몰라

	- 서버 개발자는 c/s 구조로 뭘 할 때… 연결 상태를 지속적으로 <span style='color:red'>**확인**</span>해야 함

	- Hearbeat

	- 3-way로 연결하는데, 연결이 판단된 이후에 이 채널이 유지되는지 확인해야 함

	- 슬립에 빠지지 말고 나와 통신 상태를 유지하기 위해 야 자냐? 묻자

	- **주기 → hearbeat의 주기**

- 인프라 수준에서 발생하는 비정상은 인지하기 어렵다 

- 클라, 서버나 프로세스가 kill 당했다 → RST이 날라옴 → 소켓 끊어지고, 연결 끊어진게 감지됨

	- 이 조차도 안오는게 정전과 같은 상태 

	- heatbeat를 넣어서 확인하는 수 밖에 없다 (연결 확인) 

	- 데이터 통신을 할 때, 관리적 측면이라서 자주 주고받으면 안된다. 

	- 데이터 통신 원활할 땐 하트비트 안보내도 됨

	- 효율을 높이는 설계 필요함

<br/>

**게임** 

- 고성능에 예민

- 주기 어떻게 가져갈건지, 게임 특성에 따라 차이 날 것… 

<br/>

**TCP 연결이라는 착각**

- 리셋, 블루스크린, 정전 어떻게 알아.. 

- 어플리케이션 프로토콜 설계하고 있다면, 연결은 착각이라는 대전제를 깔고 간다 

- 실험 : pc 2대, L2 스위치 , 유선연결.. 

-  랜선 뽑았다 다시 꽂으면 잘 됨 → 우리 코드가 간단하니까.

<br/>

<span style='color:red'>**연결을 신뢰할 수 없다**</span> 

- 지속적 상태 점검 (hearbeat) 필수 

<br/>

## 파일 송/수신과 프로토콜 설계

### 파일 송/수신 서비스 구조 설계 

- 어떻게 프로토콜을 디자인할까? 

- send() + recv() 는 1;1로 일치하지 않는다 → 프로토콜 정의할 때, send 한번에 recv 가 여러번으로 나뉜다.

- recv 함수의 본질은 read → 프로토콜을 입혔을 땐, **끊어낸다 라는 표현 **

	- stream 형태의 데이터 → 쭉 이어져있는 데이터 → 보내는 쪽은 stream을 보내는 느낌으로 전송

	- 받는 측은 끊어낸다는 느낌으로 받기 (댕강댕강 자르기) 

	- 데이터는 이어져서 온다. 

	- 한방에 묶어서 보낼 수 있으면 그렇게 하자

	- 수신할 땐 <span style='color:red'>**끊어낸다.**</span>

- 파일 송수신 땐, header(이름, 크기, hash)

	- 프로토콜엔 기본헤더 + 확장 헤더로 나눠서 정의

	- 보낼 땐 기본 헤더, 확장 헤더 묶여서 하고, recv 때 끊어낸다

- 기본헤더는 이동, 확장헤더는 좌표 이런 식임 (ex. GAME) 

- 프로토콜은 문자열 아님. 웹은 http 기준이라서 대부분 문자열로 프로토콜 정의되어 있음

- c/c++은 바이너리 데이터 다룬다는 관점으로.. 

- 웹에선 json 

- http/json 아니면 기본헤더, 확장헤더

- 코드 값에 따른 switch-case : lookup table 사용하면 성능 극대화 가능 

<br/>

파일 → Header + Payload

Header → code+size

- 끊어낸다 = 데이터 송신 시, 처음부터 끝까지 한번에 송신, recv 시, header 읽고 → 확장헤더 읽기 

- send는 한번이어도 recv는 n번에 걸쳐 

- 어플리케이션 프로토콜 디자인은 위의 방식임

- 파일 데이터만 보내다가 이름, 크기 보냈는데 이건 한개일 경우이고, 목록이면? 선택에 따라 값이 달라지고… 이런걸 프로토콜로 정의해서 넣겠다 

<br/>

**끊어낸다** 

- 보낼 땐 한방에 , 받을 땐 끊어낸다.

<br/>

### 응용프로그램 프로토콜 디자인  

- 끊어낸다 

	- 헤더를 주로 구조체로 정의한다. 

		- code : 정수, switch-case 처리해야 하니까, 각 케이스는 정수 

	- 확장헤더 : 구조체 정의 

	- 기본헤더

		
```c++
typedef struct MYCMD
{
	int nCode; // 명령코드
	int nSize; // 데이터의 바이트 단위 크기
} MYCMD;
```

		- 명령코드로 확장헤더 해석함. 

		- 사이즈 값 : 없으면 데이터 제대로 왔는지 판단어려움 

		- stream으로 들어오는 데이터를 읽기 위한 가장 처음 recv는 recv(&MYCMD) → 읽어서 switch-case를 보고 그 뒤를 읽어나간다

	- 확장 헤더

		
```c++
// 확장헤더 : 에러 메세지 전송 헤더
typedef struct ERRORDATA
{
	int nErrorCode;    // 에러 코드 : 향후 확장을 위한 멤버
	char szDesc[256];  // 에러 내용 
} ERRORDATA;

// 확장헤더 : S->C : 파일 리스트 전송
typedef struct SEND_FILELIST
{
	unsigned int nCount; // 전송할 파일 정보
} SEND_FILELIST;
```

	<br/>

	- **코드값**

		
```c++
// MYCMD 구조체의 nCode 멤버에 적용될 수 있는 값
typedef struct CMDCODE{
	CMD_ERROR = 50,         // 에러
	CMD_GET_LIST = 100,     // C->S : 파일 리스트 요구
	CMD_GET_FILE,           // C->S : 파일 전송 요구, 101
	CMD_SND_FILELIST = 200, // S->C : 파일 리스트 전송
	CMD_BEGIN_FILE          // S->C : 파일 전송 시작을 알림, 201
} CMDCODE;
```

		- cs 환경이니까, 양쪽에서 오갈 수 있는건지, 방향성 있는건지 주석에 작성하기 (문서작업) 

		- 설계하면, 설계는 공유해야하니까 코드 상의 주석으로 드러나기도 하지만, 문서로도 잘 정리하자 

		- 헤더는 다 구조체로 정의됨

		- 코드 값으로 이어지는 데이터를 해석할 방법을 알아냄 → 성능 좋음.. 

	- 운영 시 발생하는 장애의 분석과 모니터링 시 긍적적.

	<br/>

### 프로토콜이 적용된 파일 송신 서버 제작 


```c++
// 클라이언트로부터 명령을 수신하고 대응하는 Event loop
MYCMD cmd;
while (::**recv**(hClient, (char*)&cmd, sizeof(MYCMD), 0) > 0)
{
	switch (cmd.nCode)
	{
	case CMD_GET_LIST:
		puts("클라이언트가 파일목록을 요구함.");
		SendFileList(hClient);
		break;
	case CMD_GET_FILE:
		puts("클라이언트가 파일전송을 요구함.");
```

- 클라이언트로부터 Request가 오면 서버가 대응하는 방식

- recv로 도는 루프 : (char*)&cmd, sizeof(MYCMD) 

- switch case는 lookup table로 변경해 성능 향상 가능 

- SendFileList : 소켓을 핸들로 넘기고, 내부에서 또 다른 recv 발생

- 명령 코드에 따라 switch case 증가함

<br/>

- 서버에서 클라로 파일 보낼 때 → 목록 주고 클라가 하나 선택했다면

	- send 여러 번 해도 합쳐서 보냈을 가능성 높음 (네이글 알고리즘) 

<br/>

### 프로토콜이 적용된 파일 수신 클라이언트 제작 


```c++
if (::connect(hSocket, (SOCKADDR*)&svraddr, sizeof(svraddr)) == SOCKET_ERROR)
	ErrorHandler("서버에 연결할 수 없습니다.");

// 서버로부터 파일 리스트를 수신한다
GetFileList(hSocket);
// 전송받을 파일을 선택하고 수신한다
GetFile(hSocket);
```

- 연결 성공 시, 파일 목록 수신 후 파일 선택하여 수신 

- 클라이언트는 Active, 서버는 Passive

<br/>


```c++
void GetFileList(SOCKET hSocket)
{
	//서버에 파일 리스트 요청
	MYCMD cmd = { CMD_GET_LIST, 0 };
	::send(hSocket, (const char*)&cmd, sizeof(cmd), 0);
	
	// 서버로부터 파일 리스트 수신
	::recv(hSocket, (char*)&cmd, sizeof(cmd), 0);
	if (cmd.nCode != CMD_SND_FILELIST)
		ErrorHandler("서버에서 파일 리스트 수신 불가함.");
		
	SEND_FILELIST filelist; 
	::recv(hSocket, (char*)&filelist, sizeof(filelist), 0);
```

- 파일 목록 요구 

- 명령 코드 확인해보고 일치하지 않으면 처리 (if문) 

- 서버에서 send 한 것 끊어내서 목록 뽑아내기 

<br/>

<span style='color:red'>**프로토콜 적용 파일 클라이언트 - 요청**</span>


```c++
// 1. 서버에 파일 전송을 요청 
BYTE *pCommand = **new BYTE[sizeof(MYCMD) + sizeof(GETFILE)];**
memset(pCommand, 0, sizeof(MYCMD)+sizeof(GETFILE));

MYCMD *pCmd = (MYCMD*)pCommand; 
pCmd->nCode = CMD_GET_FILE;
pCmd->nSize = sizeof(GETFILE);

<span style='color:orange_background'>**GETFILE *pFile = (GETFILE*)(pCommand + sizeof(MYCMD));
pFile->nIndex = nIndex; 
// 두 헤더를 한 메모리에 묶어서 전송
::send(hSocket, (const char*)pCommand, sizeof(MYCMD) + sizeof(GETFILE), 0);
delete [] pCommand; **</span>
```

- send, recv를 다 잘라서 했었는데 메모리를 동적 할당함 (여기서) 

- 연속되게 메모리 동적 할당함. 

- **new BYTE[sizeof(MYCMD) + sizeof(GETFILE)];  → 두 구조체가 저장될 수 있는 메모리를 통으로 확보함 (연속되게) **

- 0 초기화 해두고, 처음 시작점을 pCmd로 포인팅

- 이 포인터로 멤버 접근해서 코드값, 사이즈값 집어넣기 

- 그 후에 pCommand가 시작점인데, MYCMD만큼 사이즈 더하면 pFile은 코드값, 사이즈값을 입력한 바로 뒤의 위치를 가리킴

- 한덩어리 만들고 send를 묶어서 전송함 

- send는 한방에 묶는게 더 효율적

- recv는 cmd만큼 끊어서 읽어서 확인한 뒤에 뒤 처리를 결정해야 하므로 끊어내듯 읽어야 함

- 수신한 서버는 끊어서 읽어서 해석하여 처리한다 → 문제될 것 없음

⇒ <span style='color:orange_background'>**두 헤더를 한 메모리에 묶어서 전송!**</span>

<br/>

### 파일 송수신 테스트 

- 프로토콜이 적용된 코드. 

- wireshark

	- 3-way handshake 완료

	- [PSH, ACK]에 데이터 → 64 (16진수) → 10진수로는 100. CMDCODE의 100은 CMD_GET_LIST (C→S, 파일 리스트 요구 코드) 

<br/>

→ 게임 프로토콜도 이게 보편적임 

<br/>

## 서버 성능 개선 IOCP

### 이벤트 개발 비동기 파일 입/출력 

io completion port 

io에서 입출력 완료

- 사전 선행

	- 파일에 대한 io를 뜻함

	- 파일 입출력에 대한 완료를 비동기 처리

	- 모든 입출력의 주체는 OS

	- 유저모드 어플리케이션 프로세스 개발 중인데, 이건 항상 OS에게 입출력에 대해 Request를 보냄. 내가 직접 하는게 아님

	- 운영체제는 큐를 갖고 있고, 여기에서 하나씩 끄집어내서 (io request를 담고 있음) 루프를 돌면서 하나씩 실질적 하드나 어디에 write, read 함

	- 디스크에 엑세스 제어하든 .. 그런 행위 모두 OS 레벨에서 수행

	- 운영체제에게 입출력 해달라는 요청 하면 됨

	- 프로세스 코드를 작성하는 우리는 리퀘스트를 보낸 뒤에? 

		- Complete 될 때 까지 우리는 wait 하는게 적절한가? 

			→ 내 선택이지

			완료 전에 할거 없으면 기다리는데, GUI가 있던가, 여러 io 동시 발생하면(서버) wait하면 절대 안됨 ⇒ <span style='color:red'>**비동기**</span>

		- os에 입출력 요청은 짧지만, 처리엔 좀 더 오랜 시간

			- 중첩은 어쩔 수 없음. 요청이 너무 쉬우니까. → 3개의 io request 요청은 짧지만, 모든 처리는 5분, 10분 걸릴 수도 

- 운영체제에게 request 전송

	- write가 read보다 3-4배 느려 

- 파일은 보조기억장치 사용

- 생성 땐, size=0, 여기에 write하면 크기 늘어남 → io가 일어나는 위치(포인터)가 자동으로 증가함. 

	- 상대위치, 절대위치 → 입출력 요구 시, OS에게 입출력해야 할 데이터 메모리에 보내야 할 정보 (1. 위치정보(파일의 시작점으로부터의 위치), 2. 사이즈)

- fopen_s() : 는 비동기 불가, <span style='color:red'>**CreateFile API**</span> 사용해야 함 

- <span style='color:red'>**CreateFile () : **</span>Open도 하고, 실제로 Create도 하는 것. 

	
```c++
// 중첩된 쓰기 속성을 부여하고 파일 생성
HANDLE hFile = ::***CreateFile***(
		_T("TestFile.txt"),
		GENERIC_WRITE, // 쓰기 모드 
		0,  // **공유**하지 않음 (프로세스 간 공유)
		NULL, 
		CREATE_ALWAYS, // 무조건 생성 (이름 겹치는 경우)
		FILE_ATTRIBUTE_NORMAL | **FILE_FLAG_OVERLAPPED**, // 중첩된 쓰기
		NULL);
```

	- 공유? : 파일에 대해서 프로세스 A가 읽기 위해 오픈 , B도 열어 → 읽기만 할거면 허용함. 원래 안됨 (수정하면…) 

	- FILE_FLAG_OVERLAPPED : 중첩된 파일 입출력 할 때 필요함 

		- io 한번에 여러개 

<br/>

**중첩된 파일 입/출력 지원 파일 생성**

- OS가 이벤트 루프처럼 돌고 있고 큐가 있으면 request가 쌓여 있음 (request = overlapped 구조체 ) 

	
```c++
// 비동기 쓰기와 관련한 OVERLAPPED 구조체 및 이벤트 핸들을 선언한다.
DWORD dwRead;
OVERLAPPED aOl[3] = {0 };
HANDLE aEvt[3] = { 0 };

// 세 번의 비동기 쓰기 완료를 확인하기 위한 이벤트 객체를 생성
for (int i= 0; i< 3; ++i)
{
	aEvt[i] = ::CreateEvent(NULL, FALSE, FALSE, NULL);
	aOl[i].hEvent = aEvt[i];
}
```

	- 이벤트 객체 생성 ⇒ request 완료되면 이벤트 set 하겠다 

	- 이벤트 = waitforsingleobject로 이벤트가 리셋 상태에서 셋 되길 기다리는 함수.. 

	- 이벤트 핸들 3개, 리퀘스트 0 1 2 를 운영체제 큐에 넣겠다. 

<br/>


```c++
// 비동기 쓰기가 시작될 지점 기술
// 두번째 쓰기는 세번째 쓰기보다 나중에 이루어질 가능성 높음
aOl[0].Offset = 0; // 파일의 시작
aOl[1].Offset = 1024 * 1024 * 5; // 5MB -> 이게 늘어나면 2는 가장 나중에 실행될 것. OS가 공간 확보하는데 시간 오래걸리니까 
aOl[2].Offset = 16; // 16 바이트

// 세번의 비동기 쓰기를 순차적 수행
for(int i=0; i<3; ++i)
{
	printf("%d번째 중첩된 쓰기 시도\n", i);
	::WriteFile(hFile, "0123456789", 10, &dwRead, &aOl[i]);
	// 정상적인 경우 쓰기 시도는 지연(보류) 된다!
	if (::GetLastError() != ERROR_IO_PENDING) exit(0);
}
```

- Offset : 파일의 시작점을 기준으로 하고, 떨어진 곳을 위치로 특정

- 세 IO 정보로 , offset의 위치 지정해둠 → offset은 위치 정보

- 파일은 처음 생성하면 크기 0인데, 0을 확 늘려서 1GB라 하면, 운영체제는 보조기억장치 메모리 확보하라는 명령 받은 것 → 오랜시간 소요됨 (OS)

- 중첩하기로 했으니까 write 해버리면 → OS에 부탁 → 3번 루프 → write 세번 → 바로 리턴하는게 아니고, GetLastError 호출해서 **Io Pending**됨 

	= 운영체제의 큐에 있는 리퀘스트를 끄집어내서 이벤트 루프에서 처리 수행하는데, 그 처리를 수행하기 위해 큐에 잘 들어감 = 접수가 잘 됨 = 그때가 IO Pending 상태 (io가 큐에 잘 들어갔다) 

	- ≠ ERROR_IO_PENDING 상태는 정상임. 중첩 IO 일 때 → 만약 펜딩 아니라면 접수 안되었단 말

<br/>


```c++
// 세 번의 비동기 쓰기가 완료되길 대기
DWORD dwResult = 0;
for(int i=0; i<3; ++i)
{
	dwResult = ::WaitForMultipleObjects(3, aEvt, FALSE, INFINITE);
	printf("-> %d번째 쓰기 완료.\n", dwReulst - WAIT_OBJECT_0);
}
```

- 이벤트 세개니까 루프 세번 돌기

- 처리 다 되면 request에 있던 이벤트 핸들 (3개)를 각각 운영체제가 set 해줄 것 → 3개 중 하나라도 set 되면, 함수 반환 → 3개가 모두  입출력될 때 까지 대기

<br/>

- OS가 루프 돌면서 입출력 끄집어낼 때, 쓰레드는 멀티 

- 그 중 오래걸리는 작업은 완료 시점이 뒤로 밀리겠지 

- 유저 모드 어플리케이션 수준에서, OS가 처리 모두 끝냈을 때 알려줘야지 → 야 다 끝났어 

	1. Event 방식

	1. **Callback **방식 

		- Event는 호출자 쪽에서 WaitFor 함수를 호출해서 이벤트 오는지 감시 (대부분은, wait 로직을 별도 스레드로 분리함. 연산은 메인에서 하고, 입출력 종료 감지는 다른 스레드) 

<br/>

### Callback 기반 비동기 파일 입/출력 

**Event**

	- 프로세스에서 파일 입출력해달라고 OS에 request 날림

	- 운영체제가 파일에 IO 요청하면 리퀘스트 처리해야 함. 처리 완료되면 OS는 프로세스에 request 보내고 event를 대기

	- io request 나갈 때 io overlapped 안에 이벤트 핸들 있음. 완료되면 핸들에 대고 (이벤트 오브젝트) set 함 → 프로세스가 종료 인식함

	<br/>

**중첩된 파일 입/출력 - Callback 방식**

- 이벤트 대기하는게 아니라 함수 만들어서 완료되면 함수 콜 하게 함

- 메인 함수, 생성한 스레드에서 콜백 함수를 콜하는 함수는 절대 등장하지 않음

- 운영체제가 완료되면 콜함

⇒ <span style='color:red'>**IOCP**</span>


```c++
int _tmain(int argc, _TCHAR* argv[])
{
	// 중첩된 쓰기 속성을 부여하고 파일 생성
	HANDLE hFile = ::***CreateFile***(
		_T("TestFile.txt"),
		GENERIC_WRITE, // 쓰기 모드 
		0,  // 공유하지 않음
		NULL, 
		CREATE_ALWAYS, // 무조건 생성
		FILE_ATTRIBUTE_NORMAL | ***FILE_FLAG_OVERLAPPED***, // 중첩된 쓰기
		NULL);
		
	// 비동기 쓰기를 위한 스레드 생성
	HANDLE hThread = NULL;
	DWORD dwThreadID = 0;
	
	dwThreadID = 0;
	hThread = ::CreateThread(
							NULL, // 보안속성 상속
							0, // 스택 메모리는 기본크기(1MB)
							IoThreadFunction, // 스레드로 실행할 함수 이름
							hFile, // 함수에 전달할 매개변수
							0, // 생성 플래그는 기본값 사용
							&dwThreadID); // 생성된 스레드 ID 저장

// 작업자 스레드가 종료될 때 까지 대기
::WaitForSingleObject(hThread, INFINITE);
```

- 흐름

	- main에서 스레드 생성 → 스레드 내부에서 wait 걸림 (SleepEx()) → 제3의 함수에서 ioComplete됨을  콜백해줄 함수가 역호출될 때 까지 SleepEx로 대기 

	- 운영체제는 Io Complete 함수 콜백 → 함수 끝나면 입출력 완료임

	- 콜백 완료(콜백함수종료)까지 대기하다가 SleepEx가 리턴하고 스레드 종료 

- 스레드 상태 

	- Running 

	- Suspended

	→ 둘을 왔다갔다 함

	멈췄는지, 작동인지

	SleepEx는 AlertableWait 상태가 됨

	Alertable Wait : 호출자 스레드 (SleepEx를 호출한 스레드)가 대기함수 콜하면 suspend되긴 하는데 sleep은 시간 지나면 작동하지만, 이 함수는 시간도 지나고 os가 콜백해야 깨어남

	→ 시간 다 안돼도 콜백하면 되살아남 

	⇒ SleepEx를 호출해야 Alertable Wait 상태가 됨

- 입출력 진행 처리 

	- 메모리 동적할당 하는데, 배열 하나 잡고 문자열 아무거나 저장

	- overlapped 구조체 동적 할당 

	- hEvent : 핸들(void pointer) 

	- 스레드 함수 어디에도 해제 코드 없음 → OS가 오버랩드 구조체의 주소 알려줌 → 그때 해제하면 됨 

- Callback 

	
```c++
::WriteFileEx((HANDLE)pParam,
	pszBuffer,
	sizeof(char) * 16,
	pOverlapped,
	FileIoComplete); // callback function 
	
// 비동기 쓰기 시도에 대해 Alertable wait 상태로 대기
for ( ; ::SleepEx(1, TRUE) != WAIT_IO_COMPLETION; );
```

	- io complete될 때 까지 for loop

<br/>

- main에서 스레드 하나 생성 

- 파일에 대고 스레드가 WriteFileEx 호출

- 스레드가 overlapped 구조체에 메모리 동적할당

- 프로세스에서 콜백되어야 할 io complete 함수 있음

- 운영체제가 콜백해주는데, 이때 메모리 주소 넘겨줌 → WriteFileEx를 호출하는 스레드에서 메모리를 할당

- io complete 콜백 함수 쪽에서 메모리 할당 해제 

→ 콜백은 호출자 스레드에서 alertable wait로 빠지면서 콜백 호출되길 대기함 → 호출 발생 시점은 OS가 알아서 결정

<br/>

Event 방식 vs Callback 방식

- iocp : 내부에 큐 있음 → 접근할 수 없는 큐

- 지금까진 파일에 대한 입출력이었다면 소켓으로 바꾸면 iocp 서버 

- 스레드에서 동적할당해서 OS에 넘겼는데, 처리하고 콜백해야 하는데, io 중에 프로그램이 메모리 해제하면? : 잘못된 메모리에 OS 접근 → 블루스크린

- OS가 할당한 메모리에 접근해야 하면, 운영체제는 메모리 Lock 걸어 → 유저모드 어플리케이션 때문에 시스템 os 커널 다운은 없음

- io completion 함수에 bp 걸면 컴퓨터가 아예 다운됨 → 추정) user mode application function이지만, os의 어느 지점에 bp가 걸리니까 커널 중지→ os 멈춤

<br/>

<br/>

**IOCP**

- 콜백 방식 예제 완벽히 이해 필요 

- 메모리 할당 위치되고, io complete 로직에서 해제되는 것 이해해야 함

	- 스레드에서 메모리 동적 할당

<br/>

[ 흐름 ] 

1. 프로세스의 main()에서 Thread를 생성

1. Thread에서 메모리를 잡음, 이 메모리 안에 overlapped 구조체 있고, 또 메모리 동적할당해서 값 저장..

1. 운영체제가 io complete 함수 콜백하는데, 이때 메모리의 주소도 전달

1. WriteFileEx를 호출하는 스레드에서 메모리 할당 

1. callback 함수에서 메모리 해제 

<br/>

Q. io complete 함수? 콜백 함수를 말하나? 

	→ I/O 작업 완료를 처리하는 함수가 정확히 무엇인가 

Q. overlapped 구조체? 

Q. IOCP는 메모리의 할당과 메모리의 해제 위치가 어디인가? 

<br/>

### IOCP 모델 

- Proactor 방식 고속 입/출력 모델

	- 왜 고속? → OS가 관리, 개입함

		- 입출력을 프로세스가 직접 하는게 아님. OS에게 맡겼을 때 가장 효율적 

		- 구조 관점에서 )) Proactor → 미리 뭔가 하겠단 말 → 등록해놓고 대기하다가 사전에 예비 동작 → 효율 높임 

			- 대기하다가 Request 하면 바로 출발

- 비동기 I/O 통지(Callback) 구조 

	- 파일에 writeFileEx() → 파일이 하나니까 실시하는 스레드 만듦 → 파일이 여러개라면? 작동할 스레드도 n개로 증가 → 비효율 

	- 입출력을 모든 파일이 하는건 아닐거아니야 

	- 스레드를 10개만 만들어서 처리하면 더 효율ㄹ아닐까? (동시 I/O는 동시에 10개도 안될 가능성이 높잖아) 

	- 동시다발적으로 일어날 수 있는 입출력의 개수를 생각해서 효율적 개수로.. 

	- 식당에 테이블 100개라고 알바도 100명 써?

- 사용자 요청에 대한 처리 스레드 풀을 OS가 직접 관리 

	- 파일 입출력 관리하기 위한 스레드 10개 → 현재 IO 나는게 1~2개면? 스레드 많으면 스위칭 생겨 → 안쓰는 애들은 Sleep 하고, 1~2개만 동작하게 해 

	- 이런 관리를 OS가 해준다

	- 스레드 풀 만들기, 관리, 스케줄링 다 OS가 해준다

	- OS가 하라고 하는 규칙을 철저히 따라야 함 

- 커널영역에서 사용자 메모리를 공유해 불필요한 메모리 복사 방지

	- 가상메모리라서 효율적 가능

		- 구조적 특징을 활용해서 불필요한 동작 감소

	- OS까지 같이 사용 

<br/>

**IOCP 모델**

- 입/출력 처리 시 관련 메모리에 대해 페이지 단위 Lock/Unlock 

	- OS가 관리하다 보니, 메모리(overlapped 구조체…etc) 해제되면 안되잖아 

	- OS에 뭔가 넘길 때 페이지 단위 메모리를 lock/unlock 

	- 입출력 request 내리면 바로 lock

- Callback 함수는 사용자 모드 함수이나 커널에서 호출하며, 이때마다 스위칭

	- 스위칭 → 유저모드에서 커널에 있는 코드 콜 하면 전환되어야 함 (context switching) 

	- 스위칭은 비용 소모됨 최소화되어야 함 

<br/>

[ 구조 ]

- 가상 메모리 (VMS)

- 프로세스 생성되면 VMS 가짐 → 프로세스 내부에서 IOCP 서버 만들면, 클라 소켓 100개 되면, 실제 io 처리하는 로직은 스레드가 한다. iocp는 스레드를 프로그램 시작되자마자 미리 생성함

- 준비시켜놓고 있다가 네트워크 송수신, 입출력 발생하면 스레드를 깨워서 로직 작동하게 함

- 네트워크로 보내면 write 하는데, 쓰기 위한 메모리가 VMS 상의 어떤 공간이라 하면, 그 메모리를 OS에 넘김

- Request 날라가면 iocp 내부에 iocp queue에 request가 쌓임

- os 수준에서 vms의 해당 공간에 lock → 커널 접근 중

- 가상메모리는 hdd, ssd와 ram을 하나의 논리적 메모리로 추상화함 

- NIC이 DMA 하는데, 네트워크로 데이터 오면 데이터 정리해서 RAM에 써줌 → 그럼 VMS에 바로 반영되는 것 → 메모리 복사할 필요 없음

- 그래서 iocp의 성능 좋은 것

<br/>

**vs 원래 Echo**

- send, recv 하는 에코 과정 클라이언트 하나에 대해 작동

- 여러 개로 하려면 멀티 스레드 만듦 

<br/>

**IOCP**

- CreateIOCP()

	- 커널에 IOCP Queue 생성되고, 체계 만들어짐

- 입출력 처리를 담당할 스레드 n개 **미리 **생성함 → 개수는 시스템 환경마다.. (서버 코어 개수에 따라?) 

- WSASocket() : IOCP개 개입하다보니, 전용 함수 사용해야 함

- CreateIOCP() 두번? 

	- 첫번짼 생성

	- 두번짼 accept 해서 생긴 클라이언트 소켓을 IOCP Queue에 감시해달라고 전달하는것 (묶어주느라 호출) 

- WSARecv() : ReadFileEx() 처럼.. IO는 Pending됨 → 서버는 대기함 (클라가 데이터 보낼 때 까지 ) → 스레드 내부에서 GQCS() 호출

- GQCS() : Get Queued Completion Status : IOCP 큐에 하나씩 꺼내는 것 

	- 만약 채팅 예제라면, GQCS→SendMessageAll→WSARecv()

		- WSARecv : 데이터 날라오면 알려줘

	- 다시 GQCS() 걸어서 Wait 하다가 뭔가 날라오면 대응

- 스레드 만들고 다 suspend 상태인데, OS 입장에서 깨우고 관리하는 것 다 함

- Recv 한다고 할 때, 메모리는 프로세스의 VMS 공간이 WSARecv로 전달되어서 호출되는 순간 OS가 락 걸어서 접근 못하게 

<br/>

<br/>

### IOCP 기반 채팅 서버 


```c++
typedef struct _USERSESSION
{
	SOCKET hSocket; 
	char buffer[8192];
} USERESSSION;

// 클라이언트 처리를 위한 작업자 스레드 개수
#define MAX_THREAD_CNT       4

CRITICAL_SECTION g_cs; // 스레드 동기화 객체 
std::list<SOCKET> g_listClient;  // 연결된 클라이언트 소켓 리스트
SOCKET g_hSocket;  // 서버의 리슨 소켓
HANDLE g_hIocp; // IOCP 핸들
```

- _USERSESSION : 연결된 사용자 (클라이언트) 가 사용 중인 소켓, 사용할 버퍼 ⇒ 세션 객체

	- 클라이언트 수와 객체 수는 정확히 일치할 것 (클라이언트 당 하나 부여)

- 스레드 개수 4개 → 조정 대상… 

<br/>

**IOCP 생성**


```c++
// IOCP 생서
g_hIocp = ::CreateIoCompletionPort{
	INVALID_HANDLE_VALUE, // 연결된 파일 없음
	NULL, // 기존 핸들 없음
	0, // 식별자 (key) 해당되지 않음
	0); // 스레드 개수는 OS에 맡김
if (g_hIocp == NULL)
{
	puts("ERROR: IOCP를 생성할 수 없습니다.");
	return 0;
}
```

- CreateIoCompletionPort → 매개변수 기본적으로 다 NULL 주고 생성함

	→ IOCP Queue 생성함 (관리체계 생성)

	<br/>

**처리 스레드 생성**


```c++
// IOCP 스레드들 생성
HANDLE hThread;
DWORD dwThreadID;
for (int i=0; i < MAX_THREAD_CNT; ++i)
{
	dwThreadID = 0;
	// 클라이언트로부터 문자열 수신
	hThread = ::CreateThread(NULL, // 보안속성 상속
								0, // 스택 메모리는 기본 크기 (1MB)
								ThreadComplete, // 스레드로 실행할 함수 이름
								(LPVOID)NULL,
								0, // 생성 플래그는 기본값 사용
								&dwThreadID); // 생성된 스레드 ID가 저장될 변수주소
	::CloseHandle(hThread);
}
```

- 4개 생성

- in IOCP : GQCS()로 생성하는 스레드 

- 클라이언트의 리퀘스트 처리하는 스레드 4개 생성

<br/>

**서버 소켓 생성**


```c++
// Create Server Listen Socket
g_hSocket = ::**WSASocket**(AF_INET, SOCK_STREAM, IPPROTO_TCP, NULL, 0, <u>**WSA_FLAG_OVERLAPPED**</u>);

// bind() / listen()
SOCKADDR_IN addrsvr;
addrsvr.sin_family = AF_INET;
addrsvr.sin_addr.S_un.S_addr = ::htonl(INADDR_ANY);
addrsvr.sin_port = ::htons(25000);
```

- **WSASocket : **IOCP 소켓이라서 OVERLAPPED를 지원해야 함 

<br/>

**접속대기**


```c++
// 반복해서 클라이언트의 연결을 accept 한다
hThread = ::CreateThread(NULL, 0, **ThreadAcceptLoop**, (LPVOID)NULL, 0, &dwThreadID);
::CloseHandle(hThread);

// _tmain() 함수가 반환하지 않도록 대기
puts("*** 채팅서버를 시작합니다! ***");
while (1)
	getchar();
```

- accept 하는걸 스레드로 분리함

	- GQCS 호출하는 작업자 스레드 만들어두고 메인에선 아무것도 안함 

		
```c++
DWORD WINAPI **ThreadAcceptLoop**(LPVOID pParam) // 클라이언트 연결 받아서 accept만 하는 스레드 
{
	LPWSA**OVERLAPPED** pWol = NULL;
	DWORD           dwReceiveSize, dwFlag;
	USERSESSION     *pNewUser;
	int             nAddrSize = sizeof(SOCKADDR);
	WSABUF          wsaBuf;
	SOCKADDR        ClientAddr;
	SOCKET          hClient; 
	int             nRecvResult = 0;
}
```

		
```c++
while ((hClient = ::**accept**(g_hSocket, &ClientAddr, &nAddrSize)) != INVALID_SOCKET)
{
	puts("새 클라이언트가 연결됐습니다.");
	::EnterCriticalSection(&g_cs);
	g_listClient.push_back(hClient);
	::LeaveCriticalSection(&g_cs);
	
	// 새 클라이언트에 대한 세션 객체 생성
	pNewUser = new USERSESSION;
	::ZeroMemory(pNewUser, sizeof(USERSESSION));
	pNewUser->hSocket = hClient; 
```

		- 새 클라이언트 연결되면 통신소켓 생성됨 → USERSESSION에 담아서 통신하는데 필요한 정보 관리함.

<br/>

**IOCP 연결** 

	
```c++
// 비동기 수신 처리를 위한 OVERLAPPED 구조체 생성
pWol = new WSAOVERLAPPED;
::ZeroMemory(pWol, sizeof(WSAOVERLAPPED));

//연결된 클라이언트 소켓 핸들을 IOCP에 연결
::CreateIoCompletionPort((HANDLE)hClient, g_hIocp,
							(ULONG_PTR)pNewUser, //KEY~!!!
							0);
```

	→ 메모리 동적할당

	- OVERLAPPED 구조체 생성 → 비동기 파일 입출력 역할

	- hClient : 방금 accept 함수가 반환한 소켓을 IOCP에 관리해달라고 엮기 

	- g_hIocp : 따라가면 IOCP Queue가 나옴, hClient를 IOCP가 관리해야 하는걸로 등록

	- KEY : 유저 연결될 때 마다 관리 정보 담긴 구조체 (메모리 동적할당) → malloc() 주소 반환할텐데, 이 주소를 키로 씀 

		- io complete 콜백 함수에서 정보전달받기 용이 

		- 키를 동적할당한 메모리 전달함

	- VMS 부분 하나가 전달되었다고 생각하면 됨 

<br/>

**수신대기**


```c++
dwReceiveSize = 0;
dwFlag = 0;
wsaBuf.buf = pNewUser->buffer;
wsaBuf.len = sizeof(pNewUser->buffer);

//클라이언트가 보낸 정보를 비동기 수신
nRecvResult = ::**WSARecv**(hClient, &wsabuf, 1, &dwReceiveSize, &dwFlag, pWol, NULL);
if (::WSAGetLastError() != WSA_IO_PENDING)
	puts("ERROR: WSARecv() != WSA_IO_PENDING");
```

- wsaBuf : IOCP에서 메모리 쓸 때, 이 메모리 아무도 못쓰게 락 할 때 관련 정보 담는 구조체

	- 사용자 정보의 메모리를 buf에 담음 (IOCP에 걸어주기) 

- WSARecv : IOCP에 소켓 등록된 후에 소켓 쪽으로 데이터 날라오면 IOCP Queue가 스레드 콜백해주라는 구조 

	- 리시브를 걸었다고 표현 → IOCP에게 입출력 올 것 같으면 알려줘~ 라고 걸기 → 안걸어주면 데이터 와도 콜백 안됨

<br/>

비즈니스 로직 들어갈 스레드 미리 만드는데, 

- ThreadComplete : GQCS()로 호출한 스레드 의미함. 

	- GetQueuedCompletionStatus() = GQCS() → IOCP 큐에 뭔가 와서 쌓이면 그 결과물을 하나씩 퍼올리는 함수 

		- 퍼올리려면 스레드가 연산하고 있어야 하니까 OS가 쓰레드 깨워주기 (Alertable Wait 상태였다가 OS가 비동기로 깨워주기)

		
```c++
puts("[IOCP 작업자 스레드 시작]");
while (1)
{
	bResult = ::GetQueuedCompletionStatus(
			g_hIocp, //Dequeue할 IOCP 핸들
			&dwTransFerredSize, // 수신한 데이터 크기
			(PULONG_PTR)&pSession, // 수신된 데이터가 저장된 메모리
			&pWol, // OVERLAPPED 구조체
			INFINITE); // 이벤트를 무한정 대기
```

		- 함수 콜 하면서 Alertable wait 하기도 핳고, 뭔가 받아오기도 함

		- IOCP 핸들은 처음 IOCP 생성할 때 받은 핸들

		- 송수신된 데이터 크기 , OVERLAPPED, 키 정보(pSession) 

<br/>

[ cf ]

- 넘어가는 키값 pNewUser, completion 하는 측에서 받음

-  IOCP 큐에 의도적으로 넣는건 Post() 

- 아직 처리하지 못한 Reqeust가 있을 때 종료하고 싶으면 cancle() 

<br/>

<br/>

## UDP와 브로드캐스트

### UDP 프로토콜 특징

- 연결, 상태 개념이 없음

- 흐름제어, 송/수신 보장 혹은 확인에 관한 기능 없음

	- 흐름제어 없음 = 송수신하든 상태가 어떻게 되든, Peer A Peer B 간에 데이터 주고받는데, 인터넷 불안정할 수 있잖아 → 유실될 수도, A에게 여유 없어서 못받아 → 통신 시엔 UDP 좋음 

	- 기능 없어서 흐름제어 안하니까 연결, 상태 없음 

	- 보낼게 받아. 근데 유실되거나, 못받아도 난 몰라 

	- 속도가 UDP >> TCP 

	- UDP 각광받음 

	- 게임서버, 클라이언트 → TCP 연결되 어 있다면, 클라가 정보 보냈을 때 동기화 되어야 함 → ㅡㄹ라 생성 정보 중에 몇 개의 로스 나더라도 상관 없는 .. 100개 중에 한명이라도 느려지면 나머지도 느려져 → TCP로 만들면 모든 데이터가 안정적으로 보내길 바라지만, 좀 버려져도 상관없는 데이터  → UDP가 더 나음. 빠른속도 

	- 영상전송 : MP4 → 실시간 전송 가능한 .. → 중간에 몇 패킷 놓치더라도 화면 잠깐 뭐 찌그러지거나..해도 되니까 

		- 일부 손실하더라도 빨리 보내는게 더 중요 

- 개발자 스스로 TCP 구현 가능

- sendto, recvfrom 밖에 없음

<br/>

### UDP 송/수신

소켓 생성


```c++
// SOCK_DGRAM 타입 사용. TCP는 STREAM
SOCKET hSocket = ::socket(AF_INET, **SOCK_DGRAM**, **IPPROTO_**<span style='color:red'>**UDP**</span>);
if (hSocket == INVALID_SOCKET)
	ErrorHandler("UDP 소켓을 생성할 수 없습니다");
```

<br/>

- 메인스레드, 워커스레드(sendto만 함)

- 연결이 없어서, recvfrom엔 주소 구조체,  수신 정보 저장할 메모리주소를 파라미터로.. 

- 송신 시, 연결이 없다보니 sendto() 에 ip, port를 매번 써줘야 함

	
```c++
::sendto(hSocket, szBuffer, strlen(szBuffer) +1, 0, (sockaddr*)&remoteaddr, sizeof(remoteaddr));
```

<br/>

<br/>

### UDP 브로드캐스트 송/수신 

- 브로드캐스팅 

	- 방송? 

	- 반대 : 유니캐스트

	- 지금까지 한게 유니캐스트 : 여러 클라 중 특정 하나랑 통신하자

	- 브로드캐스트 : 어느 구간 내 전부 다에게 감

	- 방송 인 이유’

	- L2 스위치 : 브로드캐스트는 L2 스위치 하나로 제한됨

	- 브로드캐스트라는 특수 주소에 대고 트래픽 송신하면 다간다 → L2  스위치에 부하 생김 → 효율저하 

	- 꼭 필요할 때만 해야 함 (부하생기니까) 


```c++
// Create Socket
SOCKET hSocket = ::socket(AF_INET, SOCK_DGRAM, IPPROTO_UDP);
if (hSocket == INVALID_SOCKET)
	ErrorHandler("UDP 소켓을 생성할 수 없습니다.");

// 포트 바인딩
SOCKADDR_IN addr = { 0 };
addr.sin_family = AF_INET;
addr.sin_port = htons(25000);
addr.sin_addr.S_un.S_addr = htonl(<span style='color:red'>**INADDR_BROADCAST**</span>);
```

- 데이터 송신은 방송주소로 보냄

- 포트번호는 25000 사용, 전체적으로  메세지 뿌림

- 25000 쪽으로 유입되는 데이터 받기

<br/>

여러번 send 할 필요없이 여럿에게 한번에 감 

<br/>

<br/>

## 추가..

### DNS 질의방법

- Domain Name Service = DNS 

- ISP 단위로 DNS 서버가 있음

- www : 호스트, naver.com : 도메인

- DNS = url을 IP로 변환해주는 서비스 

- DNS Server = 변환 서비스 제공하는 서버 


```c++
hostent *pHost = ::**gethostbyname**("www.naver.com");
```

<br/>

### 간단한 HTTP, FTP 클라이언트

- 업데이트 서버 접속해서 정보 가져와서 실행해야 할 경우.. 

- 객체 ) CInternetSession

- 웹 파일 저장이 캐시에 영향받음 

- FTP : WinINet 라이브러리 사용 

	- InternetConnect 

<br/>

### 윈도우에서 어댑터 정보 확인 방법

- 왜 확인할까? 

	- ipconfig 하면 다양한 정보 볼 수 있는데 API 함수로도 구현 가능

	- ::GetAdaptersInfo로 윈도우에서 정보 얻기 가능 (MAC 주소 얻기 가능) 

		
```c++
DWORD dwResult = ::GetAdaptersInfo(pNicInfo, &uBufferLength);
```

		⇒ NIC이 두개 이상이면 모든 정보를 담을 수 없으므로 메모리 다시 할당함 

<br/>

<br/>

([정리본](https://www.notion.so/2518b858bb3a8107b025c37e278844aa]) 

<br/>

<br/>

inet_pton(AF_INET, "127.0.0.1", &recvAddr.sin_addr);

